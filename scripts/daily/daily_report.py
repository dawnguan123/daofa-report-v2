#!/usr/bin/env python3
"""
æ¯æ—¥æ—¶äº‹æŠ¥å‘Šç”Ÿæˆ - ç®€åŒ–ç‰ˆ
"""
import yaml
import sqlite3
import json
import requests
import sys
import os
import re
from datetime import datetime

# åŠ è½½é…ç½®
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

class MinimaxClient:
    def __init__(self, api_key):
        self.api_key = api_key
    
    def get_embedding(self, text):
        """è°ƒç”¨Minimaxè·å–å‘é‡åµŒå…¥"""
        try:
            url = "https://api.minimax.chat/v1/text/embeddings"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            data = {
                "model": "text-embedding-005",
                "input": text,
                "encoding_format": "float"
            }
            response = requests.post(url, json=data, headers=headers, timeout=30)
            response.raise_for_status()
            return response.json()['data'][0]['embedding']
        except Exception as e:
            print(f"  âš ï¸ Minmax APIé”™è¯¯: {e}")
            return [0.0] * 768

def keyword_match(news_keywords, chapter_text):
    """å…³é”®è¯åŒ¹é…"""
    news_keywords = [k.lower() for k in news_keywords]
    chapter_lower = chapter_text.lower()
    
    matches = 0
    for keyword in news_keywords:
        if keyword in chapter_lower:
            matches += 1
    
    return matches

def search_chapters(news, db_path, limit=3):
    """æœç´¢ç›¸å…³è¯¾æœ¬ç« èŠ‚ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='textbook_anchors'")
        if not cursor.fetchone():
            return []
        
        cursor.execute("SELECT * FROM textbook_anchors")
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        
        results = []
        for row in rows:
            ch = dict(zip(columns, row))
            # å…³é”®è¯åŒ¹é…
            score = keyword_match(
                news['title'] + ' ' + news['summary'],
                ch.get('content', '') + ' ' + ch.get('content_summary', '')
            )
            if score > 0:
                ch['relevance'] = score
                results.append(ch)
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.get('relevance', 0), reverse=True)
        return results[:limit]
        
    except Exception as e:
        print(f"  âš ï¸ æœç´¢é”™è¯¯: {e}")
        return []
    finally:
        conn.close()

def fetch_news(date_str):
    """è·å–ä»Šæ—¥æ–°é—»"""
    sample_news = [
        {
            "title": "å…¨å›½æ•™è‚²å¤§ä¼šåœ¨åŒ—äº¬å¬å¼€",
            "source": "æ–°åç¤¾",
            "time": f"{date_str} 10:00",
            "summary": "ä¼šè®®å¼ºè°ƒå…¨é¢æ¨è¿›æ•™è‚²ç°ä»£åŒ–ï¼Œå»ºè®¾æ•™è‚²å¼ºå›½ã€‚",
            "category": "æ•™è‚²"
        },
        {
            "title": "ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹æ–°å¸æ³•è§£é‡Šå‘å¸ƒ",
            "source": "äººæ°‘æ—¥æŠ¥",
            "time": f"{date_str} 09:30",
            "summary": "æœ€é«˜äººæ°‘æ³•é™¢å‘å¸ƒå…³äºæ°‘äº‹æ¡ˆä»¶å®¡ç†çš„æœ€æ–°å¸æ³•è§£é‡Šã€‚",
            "category": "æ³•å¾‹"
        },
        {
            "title": "å›½å®¶ç»Ÿè®¡å±€å‘å¸ƒ2026å¹´1æœˆç»æµæ•°æ®",
            "source": "å¤®è§†æ–°é—»",
            "time": f"{date_str} 08:00",
            "summary": "1æœˆä»½CPIåŒæ¯”ä¸Šæ¶¨0.5%ï¼Œç»æµè¿è¡Œæ€»ä½“å¹³ç¨³ã€‚",
            "category": "æ—¶æ”¿"
        },
        {
            "title": "æ•™è‚²éƒ¨ï¼šæ¨è¿›ä¹‰åŠ¡æ•™è‚²ä¼˜è´¨å‡è¡¡å‘å±•",
            "source": "ä¸­å›½æ•™è‚²æŠ¥",
            "time": f"{date_str} 14:00",
            "summary": "æ•™è‚²éƒ¨è¦æ±‚å„åœ°åŠ å¿«æ¨è¿›ä¹‰åŠ¡æ•™è‚²ä¼˜è´¨å‡è¡¡å‘å±•ã€‚",
            "category": "æ•™è‚²"
        },
        {
            "title": "æœ€é«˜æ£€å‘å¸ƒæœªæˆå¹´äººæ£€å¯Ÿå·¥ä½œç™½çš®ä¹¦",
            "source": "æ£€å¯Ÿæ—¥æŠ¥",
            "time": f"{date_str} 11:00",
            "summary": "æŠ¥å‘Šæ˜¾ç¤ºæœªæˆå¹´äººçŠ¯ç½ªç‡åŒæ¯”ä¸‹é™15%ã€‚",
            "category": "æ³•å¾‹"
        }
    ]
    return sample_news[:config['news']['top_n']]

def generate_html(report_date, news_list, chapters_dict, pdf_url):
    """ç”ŸæˆHTMLæŠ¥å‘Š"""
    html = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>é“æ³•æ—¶äº‹æŠ¥å‘Š - {report_date}</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
</head>
<body class="bg-gray-50">
    <div id="app" class="max-w-4xl mx-auto px-4 py-8">
        <header class="text-center mb-8">
            <h1 class="text-3xl font-bold text-gray-800">ğŸ“° é“æ³•æ—¶äº‹æŠ¥å‘Š</h1>
            <p class="text-gray-500 mt-2">{report_date}</p>
        </header>
        
        <div class="space-y-6">
'''
    
    for i, news in enumerate(news_list, 1):
        chapters = chapters_dict.get(i, [])
        
        html += f'''
            <div class="bg-white rounded-lg shadow p-6">
                <h2 class="text-xl font-bold text-blue-600 mb-2">{i}. {news['title']}</h2>
                <div class="text-sm text-gray-400 mb-4">
                    <span class="bg-blue-100 text-blue-800 px-2 py-1 rounded">{news['category']}</span>
                    {news['source']} Â· {news['time']}
                </div>
                <p class="text-gray-700 mb-4">{news['summary']}</p>
'''
        
        if chapters:
            html += '''
                <div class="bg-green-50 border-l-4 border-green-500 p-4">
                    <h3 class="font-bold text-green-700 mb-2">ğŸ“š è¯¾æœ¬å…³è”</h3>
'''
            for ch in chapters:
                html += f'''
                    <div class="mb-3">
                        <div class="flex items-center gap-2">
                            <span class="bg-green-100 text-green-800 px-2 py-1 rounded text-sm">
                                {ch.get('chapter_title', 'æœªçŸ¥ç« èŠ‚')} (é¡µç  {ch.get('page_range', 'æœªçŸ¥')})
                            </span>
                        </div>
                        <p class="text-gray-600 text-sm mt-1">{ch.get('content_summary', '')[:100]}...</p>
                    </div>
'''
            html += '''
                </div>
'''
        
        html += '''
            </div>
'''
    
    html += '''
        </div>
    </div>
    <script>const { createApp } = Vue; createApp({}).mount('#app')</script>
</body>
</html>'''
    
    return html

def main():
    if len(sys.argv) > 2 and sys.argv[1] == '--date':
        report_date = sys.argv[2]
    else:
        report_date = datetime.now().strftime('%Y-%m-%d')
    
    print("=" * 60)
    print(f"ğŸ“° ç”Ÿæˆæ—¶äº‹æŠ¥å‘Š: {report_date}")
    print("=" * 60)
    
    # è·å–æ–°é—»
    print("\nğŸ“° è·å–æ–°é—»...")
    news_list = fetch_news(report_date)
    print(f"  âœ“ {len(news_list)} æ¡æ–°é—»")
    
    # åŒ¹é…è¯¾æœ¬ç« èŠ‚
    print("\nğŸ“š åŒ¹é…è¯¾æœ¬...")
    chapters_dict = {}
    for i, news in enumerate(news_list, 1):
        chapters = search_chapters(news, config['turso']['knowledge_db'], limit=3)
        chapters_dict[i] = chapters
        print(f"  âœ“ æ–°é—»{i}: {len(chapters)} ä¸ªç›¸å…³ç« èŠ‚")
    
    # ç”ŸæˆHTML
    print("\nğŸ“„ ç”ŸæˆHTML...")
    pdf_url = config['frontend']['pdf_url']
    html = generate_html(report_date, news_list, chapters_dict, pdf_url)
    
    output_dir = os.path.join(config['paths']['output_dir'], report_date)
    os.makedirs(output_dir, exist_ok=True)
    
    html_path = os.path.join(output_dir, 'index.html')
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html)
    print(f"  âœ“ {html_path}")
    
    # ç”ŸæˆJSON
    json_data = {
        "date": report_date,
        "newsCount": len(news_list),
        "news": [{"rank": i, "title": n['title'], "source": n['source'], 
                  "time": n['time'], "summary": n['summary'], 
                  "matchedChapters": chapters_dict.get(i, [])} 
                 for i, n in enumerate(news_list, 1)],
        "chapters": []
    }
    
    json_path = os.path.join(output_dir, 'report.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ {json_path}")
    
    print(f"\nâœ… å®Œæˆ! è®¿é—®: file://{html_path}")

if __name__ == "__main__":
    main()
