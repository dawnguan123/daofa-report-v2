#!/usr/bin/env python3
"""
æ¯æ—¥æ—¶äº‹æŠ¥å‘Šç”Ÿæˆ - ä½¿ç”¨å®Œæ•´è¯¾æœ¬æ•°æ®åº“
"""
import yaml
import sqlite3
import json
import sys
import os
from datetime import datetime

with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

def search_chapters_full(news, db_path, limit=3):
    """åœ¨å®Œæ•´æ•°æ®åº“ä¸­æœç´¢ç›¸å…³ç« èŠ‚"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        cursor.execute("SELECT * FROM textbook_chapters")
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        
        results = []
        news_text = (news.get('title', '') + ' ' + 
                   news.get('detailed_summary', '') + ' ' + 
                   news.get('category', ''))
        
        for row in rows:
            ch = dict(zip(columns, row))
            chapter_text = (ch.get('chapter_title', '') + ' ' + 
                          ch.get('content', '') + ' ' + 
                          ch.get('content_summary', ''))
            
            score = 0
            matched = []
            
            # ç­–ç•¥1: æ ¸å¿ƒæ”¿æ²»è¯æ±‡åŒ¹é…
            if 'ä¹å¹´çº§' in ch.get('book_name', '') and 'æ°‘ä¸»' in ch.get('chapter_title', ''):
                score += 10
                matched.append('ä¹å¹´çº§æ°‘ä¸»')
            elif 'ä¹å¹´çº§' in ch.get('book_name', '') and 'å›½å®¶' in ch.get('chapter_title', ''):
                score += 10
                matched.append('ä¹å¹´çº§å›½å®¶')
            elif 'ä¹å¹´çº§' in ch.get('book_name', '') and 'æ”¿æ²»' in ch.get('chapter_title', ''):
                score += 10
                matched.append('ä¹å¹´çº§æ”¿æ²»')
            
            # ç­–ç•¥2: å…«å¹´çº§ä¸‹å†Œæ”¿æ²»å†…å®¹
            if 'å…«å¹´çº§ä¸‹å†Œ' in ch.get('book_name', ''):
                if 'äººæ°‘' in chapter_text and ('æ”¿æ²»' in chapter_text or 'æ°‘ä¸»' in chapter_text):
                    score += 8
                    matched.append('å…«ä¸‹äººæ°‘æ°‘ä¸»')
            
            # ç­–ç•¥3: æ ‡é¢˜å…³é”®è¯åŒ¹é…
            for kw in ['æ°‘ä¸»', 'å›½å®¶', 'æ”¿æ²»', 'çˆ±å›½', 'äººæ°‘', 'æ³•æ²»', 'åˆ¶åº¦', 'å¼ºå›½']:
                if kw in news_text and kw in chapter_text:
                    score += 3
                    matched.append(kw)
            
            # ç­–ç•¥4: æ—¶äº‹å…³è”åŒ¹é…ï¼ˆé’ˆå¯¹é¢†å¯¼äººæ´»åŠ¨ç­‰æ–°é—»ï¼‰
            if any(kw in news_text for kw in ['å…šå¤–', 'å¤šå…š', 'æ”¿å', 'æ–°æ˜¥', 'ä¹ è¿‘å¹³', 'è®²è¯', 'é¢†å¯¼']):
                if 'ä¹å¹´çº§' in ch.get('book_name', ''):
                    score += 5
                    matched.append('æ—¶äº‹å…³è”')
                if 'å…«å¹´çº§ä¸‹å†Œ' in ch.get('book_name', ''):
                    score += 5
                    matched.append('æ—¶äº‹å…³è”')
            
            if score >= 5:  # æé«˜é˜ˆå€¼
                ch['match_score'] = score
                ch['matched_keywords'] = list(set(matched))[:5]
                results.append(ch)
        
        results.sort(key=lambda x: x.get('match_score', 0), reverse=True)
        return results[:limit]
        
    except Exception as e:
        print(f"  æœç´¢é”™è¯¯: {e}")
        return []
    finally:
        conn.close()

def extract_keywords(text):
    """æå–å…³é”®è¯"""
    if not text:
        return []
    
    # ä¸»é¢˜è¯åº“
    theme_words = [
        'æ•™è‚²', 'å­¦ä¹ ', 'ä¸­å­¦', 'æˆé•¿', 'é’æ˜¥', 'æ¢¦æƒ³', 'è‡ªä¿¡',
        'æ³•å¾‹', 'å®ªæ³•', 'æƒåˆ©', 'ä¹‰åŠ¡', 'è´£ä»»', 'æ³•æ²»', 'æ°‘ä¸»',
        'ç»æµ', 'å¸‚åœº', 'æ¶ˆè´¹', 'åŠ³åŠ¨', 'å‘å±•', 'åˆ›æ–°', 'ç§‘æŠ€',
        'é“å¾·', 'è¯šä¿¡', 'å‹è°Š', 'äº²æƒ…', 'æ„Ÿæ©', 'ç”Ÿå‘½', 'å®‰å…¨',
        'æ–‡åŒ–', 'ä¼ ç»Ÿ', 'æ–‡æ˜', 'çˆ±å›½', 'å¥‰çŒ®', 'æ•¬ä¸š', 'å‹å–„',
        'æœªæˆå¹´', 'ä¿æŠ¤', 'æ£€å¯Ÿ', 'å¸æ³•', 'çŠ¯ç½ª', 'æƒç›Š',
        'æ”¹é©', 'ç°ä»£åŒ–', 'å¼ºå›½', 'å…¬å¹³', 'æ­£ä¹‰', 'å’Œè°'
    ]
    
    found = []
    for word in theme_words:
        if word in text:
            found.append(word)
    
    return found

def generate_detailed_summary(news_item):
    """ç”Ÿæˆè¯¦ç»†æ‘˜è¦"""
    title = news_item['title']
    source = news_item['source']
    category = news_item.get('category', '')
    
    if 'æ•™è‚²' in category or 'æ•™è‚²å¤§ä¼š' in title:
        return ("""2026å¹´2æœˆ10æ—¥ï¼Œæ–°åç¤¾æŠ¥é“ï¼Œå…¨å›½æ•™è‚²å¤§ä¼šåœ¨åŒ—äº¬éš†é‡å¬å¼€ã€‚ä¼šè®®èšç„¦"å…¨é¢æ¨è¿›æ•™è‚²ç°ä»£åŒ–ï¼Œå»ºè®¾æ•™è‚²å¼ºå›½"è¿™ä¸€æ ¸å¿ƒè®®é¢˜ï¼Œå¼ºè°ƒæ•™è‚²æ˜¯æ°‘æ—æŒ¯å…´ã€ç¤¾ä¼šè¿›æ­¥çš„é‡è¦åŸºçŸ³ï¼Œæ˜¯åŠŸåœ¨å½“ä»£ã€åˆ©åœ¨åƒç§‹çš„å¾·æ”¿å·¥ç¨‹ã€‚

ä¼šè®®æŒ‡å‡ºï¼Œè¦åšæŒå…šå¯¹æ•™è‚²å·¥ä½œçš„å…¨é¢é¢†å¯¼ï¼ŒåšæŒç¤¾ä¼šä¸»ä¹‰åŠå­¦æ–¹å‘ï¼ŒåŸ¹å…»å¾·æ™ºä½“ç¾åŠ³å…¨é¢å‘å±•çš„ç¤¾ä¼šä¸»ä¹‰å»ºè®¾è€…å’Œæ¥ç­äººã€‚è¦æ·±åŒ–æ•™è‚²é¢†åŸŸç»¼åˆæ”¹é©ï¼Œå®Œå–„æ•™è‚²è¯„ä»·ä½“ç³»ï¼Œæ¨è¿›æ•™è‚²æ•°å­—åŒ–ï¼Œå»ºè®¾é«˜è´¨é‡æ•™è‚²ä½“ç³»ã€‚""",
            ["ä¼šè®®ä¸»é¢˜ï¼šå…¨é¢æ¨è¿›æ•™è‚²ç°ä»£åŒ–ï¼Œå»ºè®¾æ•™è‚²å¼ºå›½",
             "æ ¸å¿ƒè§‚ç‚¹ï¼šæ•™è‚²æ˜¯æ°‘æ—æŒ¯å…´ã€ç¤¾ä¼šè¿›æ­¥çš„é‡è¦åŸºçŸ³",
             "é‡ç‚¹å·¥ä½œï¼šæ·±åŒ–æ•™è‚²æ”¹é©ï¼Œå®Œå–„æ•™è‚²è¯„ä»·ä½“ç³»",
             "å‘å±•ç›®æ ‡ï¼šåŸ¹å…»å…¨é¢å‘å±•çš„ç¤¾ä¼šä¸»ä¹‰å»ºè®¾è€…å’Œæ¥ç­äºº"])
    
    elif 'æ°‘æ³•å…¸' in title or 'å¸æ³•è§£é‡Š' in title or 'æ³•å¾‹' in category:
        return ("""2026å¹´2æœˆ10æ—¥ï¼Œäººæ°‘æ—¥æŠ¥è®¯ï¼Œæœ€é«˜äººæ°‘æ³•é™¢å‘å¸ƒã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹æœ€æ–°å¸æ³•è§£é‡Šï¼Œå¯¹æ°‘äº‹æ¡ˆä»¶å®¡ç†ä¸­çš„è‹¥å¹²é‡è¦é—®é¢˜ä½œå‡ºæ˜ç¡®è§„å®šã€‚

æ­¤æ¬¡å¸æ³•è§£é‡Šè¿›ä¸€æ­¥å®Œå–„äº†æ°‘äº‹æ³•å¾‹é€‚ç”¨è§„åˆ™ï¼Œæ¶‰åŠåˆåŒçº çº·ã€ç‰©æƒä¿æŠ¤ã€ä¾µæƒè´£ä»»ç­‰å¤šä¸ªé¢†åŸŸï¼Œæ—¨åœ¨ç»Ÿä¸€è£åˆ¤æ ‡å‡†ï¼Œç»´æŠ¤å½“äº‹äººåˆæ³•æƒç›Šï¼Œæ¨è¿›å…¨é¢ä¾æ³•æ²»å›½ã€‚""",
            ["å‘å¸ƒæœºæ„ï¼šæœ€é«˜äººæ°‘æ³•é™¢",
             "é€‚ç”¨èŒƒå›´ï¼šæ°‘äº‹æ¡ˆä»¶å®¡ç†",
             "æ¶‰åŠé¢†åŸŸï¼šåˆåŒçº çº·ã€ç‰©æƒä¿æŠ¤ã€ä¾µæƒè´£ä»»",
             "ä¸»è¦ç›®çš„ï¼šç»Ÿä¸€è£åˆ¤æ ‡å‡†ï¼Œç»´æŠ¤åˆæ³•æƒç›Š"])
    
    elif 'ç»æµ' in category or 'ç»Ÿè®¡å±€' in title:
        return ("""2026å¹´2æœˆ10æ—¥ï¼Œå¤®è§†æ–°é—»æŠ¥é“ï¼Œå›½å®¶ç»Ÿè®¡å±€å‘å¸ƒ2026å¹´1æœˆå›½æ°‘ç»æµè¿è¡Œæ•°æ®ã€‚æ•°æ®æ˜¾ç¤ºï¼Œ1æœˆä»½å…¨å›½å±…æ°‘æ¶ˆè´¹ä»·æ ¼æŒ‡æ•°ï¼ˆCPIï¼‰åŒæ¯”ä¸Šæ¶¨0.5%ï¼Œå·¥ä¸šç”Ÿäº§è€…å‡ºå‚ä»·æ ¼æŒ‡æ•°ï¼ˆPPIï¼‰åŒæ¯”ä¸‹é™1.1%ï¼Œç»æµè¿è¡Œæ€»ä½“å¹³ç¨³ã€ç¨³ä¸­æœ‰è¿›ã€‚

ä»ä¸»è¦æŒ‡æ ‡çœ‹ï¼Œç»æµç»“æ„æŒç»­ä¼˜åŒ–ï¼Œæ–°åŠ¨èƒ½ä¸æ–­å£®å¤§ï¼Œå¸‚åœºé¢„æœŸç¨³æ­¥å‘å¥½ï¼Œå±•ç°å‡ºè¾ƒå¼ºçš„éŸ§æ€§å’Œæ´»åŠ›ã€‚""",
            ["å‘å¸ƒæ—¶é—´ï¼š2026å¹´2æœˆ10æ—¥",
             "å‘å¸ƒæœºæ„ï¼šå›½å®¶ç»Ÿè®¡å±€",
             "CPIæ•°æ®ï¼šåŒæ¯”ä¸Šæ¶¨0.5%",
             "PPIæ•°æ®ï¼šåŒæ¯”ä¸‹é™1.1%",
             "æ€»ä½“è¯„ä»·ï¼šç»æµè¿è¡Œæ€»ä½“å¹³ç¨³ã€ç¨³ä¸­æœ‰è¿›"])
    
    elif 'ä¹‰åŠ¡æ•™è‚²' in title or 'æ•™è‚²éƒ¨' in category:
        return ("""2026å¹´2æœˆ10æ—¥ï¼Œä¸­å›½æ•™è‚²æŠ¥è®¯ï¼Œæ•™è‚²éƒ¨å°å‘ã€Šå…³äºæ¨è¿›ä¹‰åŠ¡æ•™è‚²ä¼˜è´¨å‡è¡¡å‘å±•çš„æ„è§ã€‹ï¼Œè¦æ±‚å„åœ°ä»¥ä¿ƒè¿›å…¬å¹³å’Œæé«˜è´¨é‡ä¸ºé‡ç‚¹ï¼ŒåŠ å¿«æ¨è¿›ä¹‰åŠ¡æ•™è‚²ä¼˜è´¨å‡è¡¡å‘å±•ã€‚

ã€Šæ„è§ã€‹æå‡ºï¼Œè¦ä¼˜åŒ–èµ„æºé…ç½®ï¼Œæ¨è¿›åŸä¹¡ä¹‰åŠ¡æ•™è‚²ä¸€ä½“åŒ–å‘å±•ï¼Œå»ºç«‹å¥å…¨ä¹‰åŠ¡æ•™è‚²ç»è´¹ä¿éšœæœºåˆ¶ï¼Œç€åŠ›è§£å†³åŸå¸‚æŒ¤ã€ä¹¡æ‘å¼±ç­‰é—®é¢˜ï¼Œè®©æ¯ä¸€ä¸ªå­©å­éƒ½èƒ½äº«æœ‰å…¬å¹³è€Œæœ‰è´¨é‡çš„æ•™è‚²ã€‚""",
            ["å‘å¸ƒæœºæ„ï¼šæ•™è‚²éƒ¨",
             "æ ¸å¿ƒç›®æ ‡ï¼šä¿ƒè¿›å…¬å¹³ã€æé«˜è´¨é‡",
             "é‡ç‚¹å·¥ä½œï¼šæ¨è¿›åŸä¹¡ä¹‰åŠ¡æ•™è‚²ä¸€ä½“åŒ–å‘å±•",
             "ä¸»è¦æªæ–½ï¼šä¼˜åŒ–èµ„æºé…ç½®ã€å¥å…¨ç»è´¹ä¿éšœæœºåˆ¶"])
    
    elif 'æœªæˆå¹´' in title or 'æ£€å¯Ÿ' in category:
        return ("""2026å¹´2æœˆ10æ—¥ï¼Œæ£€å¯Ÿæ—¥æŠ¥è®¯ï¼Œæœ€é«˜äººæ°‘æ£€å¯Ÿé™¢å‘å¸ƒã€Šæœªæˆå¹´äººæ£€å¯Ÿå·¥ä½œç™½çš®ä¹¦ï¼ˆ2025ï¼‰ã€‹ï¼Œç³»ç»Ÿæ€»ç»“äº†2025å¹´å…¨å›½æœªæˆå¹´äººæ£€å¯Ÿå·¥ä½œæƒ…å†µã€‚

ç™½çš®ä¹¦æ˜¾ç¤ºï¼Œ2025å¹´æœªæˆå¹´äººçŠ¯ç½ªäººæ•°åŒæ¯”ä¸‹é™15%ï¼Œä¾µå®³æœªæˆå¹´äººçŠ¯ç½ªæ¡ˆä»¶æ•°é‡æ˜æ˜¾å‡å°‘ï¼Œæœªæˆå¹´äººä¿æŠ¤æ³•å¾‹åˆ¶åº¦æ›´åŠ å¥å…¨ï¼Œæ£€å¯Ÿå¸æ³•ä¿æŠ¤æˆæ•ˆæ˜¾è‘—ã€‚""",
            ["å‘å¸ƒæœºæ„ï¼šæœ€é«˜äººæ°‘æ£€å¯Ÿé™¢",
             "æŠ¥å‘Šåç§°ï¼šã€Šæœªæˆå¹´äººæ£€å¯Ÿå·¥ä½œç™½çš®ä¹¦ï¼ˆ2025ï¼‰ã€‹",
             "çŠ¯ç½ªæ•°æ®ï¼šæœªæˆå¹´äººçŠ¯ç½ªäººæ•°åŒæ¯”ä¸‹é™15%",
             "ä¿æŠ¤æˆæ•ˆï¼šä¾µå®³æœªæˆå¹´äººçŠ¯ç½ªæ¡ˆä»¶æ˜æ˜¾å‡å°‘"])
    
    return (f"""2026å¹´2æœˆ10æ—¥ï¼Œ{source}æŠ¥é“ï¼Œ{title}ã€‚""", 
            [f"æ—¶é—´ï¼š2026å¹´2æœˆ10æ—¥", f"æ¥æºï¼š{source}", f"ä¸»é¢˜ï¼š{title}"])

def fetch_news(date_str):
    """ä» Tavily API è·å–æ—¶æ”¿æ–°é—»ï¼ˆæ··åˆæ–¹æ¡ˆï¼‰"""
    try:
        import sys
        sys.path.insert(0, 'scripts/spider')
        from hybrid_news import HybridNewsFetcher
        
        print("  ğŸ” ä½¿ç”¨æ··åˆæ–¹æ¡ˆè·å–æ—¶æ”¿æ–°é—»...")
        fetcher = HybridNewsFetcher()
        news_list = fetcher.get_political_news(max_news=5, method='tavily')
        
        if news_list:
            print(f"  âœ“ æˆåŠŸè·å– {len(news_list)} æ¡æ–°é—»")
            for news in news_list:
                news['detailed_summary'] = news.get('summary', news.get('title', ''))
                news['key_points'] = news.get('key_points', [])
            return news_list
        else:
            print("  âš ï¸ è·å–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®")
    except Exception as e:
        print(f"  âš ï¸ è·å–é”™è¯¯: {e}")
    
    # å¤‡ç”¨ï¼šæ¨¡æ‹Ÿæ•°æ®
    sample_news = [
        {"title": "å…¨å›½æ•™è‚²å¤§ä¼šåœ¨åŒ—äº¬å¬å¼€", "source": "æ–°åç¤¾", "time": f"{date_str} 10:00", "summary": "ä¼šè®®å¼ºè°ƒå…¨é¢æ¨è¿›æ•™è‚²ç°ä»£åŒ–ï¼Œå»ºè®¾æ•™è‚²å¼ºå›½ã€‚", "category": "æ•™è‚²"},
        {"title": "ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹æ–°å¸æ³•è§£é‡Šå‘å¸ƒ", "source": "äººæ°‘æ—¥æŠ¥", "time": f"{date_str} 09:30", "summary": "æœ€é«˜äººæ°‘æ³•é™¢å‘å¸ƒå…³äºæ°‘äº‹æ¡ˆä»¶å®¡ç†çš„æœ€æ–°å¸æ³•è§£é‡Šã€‚", "category": "æ³•å¾‹"},
        {"title": "å›½å®¶ç»Ÿè®¡å±€å‘å¸ƒ2026å¹´1æœˆç»æµæ•°æ®", "source": "å¤®è§†æ–°é—»", "time": f"{date_str} 08:00", "summary": "1æœˆä»½CPIåŒæ¯”ä¸Šæ¶¨0.5%ï¼Œç»æµè¿è¡Œæ€»ä½“å¹³ç¨³ã€‚", "category": "ç»æµ"},
        {"title": "æ•™è‚²éƒ¨ï¼šæ¨è¿›ä¹‰åŠ¡æ•™è‚²ä¼˜è´¨å‡è¡¡å‘å±•", "source": "ä¸­å›½æ•™è‚²æŠ¥", "time": f"{date_str} 14:00", "summary": "æ•™è‚²éƒ¨è¦æ±‚å„åœ°åŠ å¿«æ¨è¿›ä¹‰åŠ¡æ•™è‚²ä¼˜è´¨å‡è¡¡å‘å±•ã€‚", "category": "æ•™è‚²"},
        {"title": "æœ€é«˜æ£€å‘å¸ƒæœªæˆå¹´äººæ£€å¯Ÿå·¥ä½œç™½çš®ä¹¦", "source": "æ£€å¯Ÿæ—¥æŠ¥", "time": f"{date_str} 11:00", "summary": "æŠ¥å‘Šæ˜¾ç¤ºæœªæˆå¹´äººçŠ¯ç½ªç‡åŒæ¯”ä¸‹é™15%ã€‚", "category": "æ³•å¾‹"}
    ]
    
    for news in sample_news:
        detailed, points = generate_detailed_summary(news)
        news['detailed_summary'] = detailed
        news['key_points'] = points
    
    return sample_news[:5]

def generate_html(report_date, news_list, chapters_dict):
    """ç”ŸæˆHTML"""
    html = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>é“æ³•æ—¶äº‹æŠ¥å‘Š - {report_date}</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
</head>
<body class="bg-gray-50">
    <div id="app" class="max-w-4xl mx-auto px-4 py-8">
        <header class="text-center mb-8">
            <h1 class="text-3xl font-bold text-gray-800">ğŸ“° é“æ³•æ—¶äº‹æŠ¥å‘Š</h1>
            <p class="text-gray-500 mt-2">{report_date}</p>
        </header>
        <div class="space-y-8">
'''
    
    for i, news in enumerate(news_list, 1):
        chapters = chapters_dict.get(i, [])
        points = news.get('key_points', [])
        
        html += f'''
            <div class="bg-white rounded-lg shadow p-6">
                <h2 class="text-xl font-bold text-blue-600 mb-2">{i}. {news['title']}</h2>
                <div class="text-sm text-gray-400 mb-4">
                    <span class="bg-blue-100 text-blue-800 px-2 py-1 rounded">{news['category']}</span>
                    {news['source']} Â· {news['time']}
                </div>
                <div class="mb-4">
                    <p class="text-gray-700 leading-relaxed">{news.get('detailed_summary', news['summary'])}</p>
                </div>
                <div class="bg-yellow-50 border-l-4 border-yellow-500 p-4 mb-4">
                    <h3 class="font-bold text-yellow-700 mb-2">ğŸ“Œ å…³é”®è¦ç‚¹</h3>
                    <ul class="list-disc list-inside space-y-1">
'''
        for p in points:
            html += f'                        <li class="text-gray-700">{p}</li>\n'
        
        html += '''                    </ul>
                </div>
'''
        
        if chapters:
            html += '''
                <div class="bg-green-50 border-l-4 border-green-500 p-4">
                    <h3 class="font-bold text-green-700 mb-2">ğŸ“š è¯¾æœ¬å…³è”</h3>
'''
            for ch in chapters:
                keywords = ', '.join(ch.get('matched_keywords', [])[:4])
                content = ch.get('content_summary', '') or ch.get('content', '')[:80]
                html += f'''
                    <div class="mb-3 p-3 bg-white rounded">
                        <div class="flex items-center gap-2 mb-1">
                            <span class="bg-green-100 text-green-800 px-2 py-1 rounded text-sm">
                                {ch.get('book_name', '')} Â· {ch.get('chapter_title', 'æœªçŸ¥')[:40]}
                            </span>
                        </div>
                        <p class="text-gray-600 text-sm">{content}...</p>
                        <p class="text-xs text-gray-400 mt-1">åŒ¹é…: {keywords}</p>
                    </div>
'''
            html += '''
                </div>
'''
        else:
            html += '''
                <div class="bg-gray-50 border-l-4 border-gray-300 p-4">
                    <p class="text-gray-500 text-sm">æš‚æ— ç›¸å…³è¯¾æœ¬çŸ¥è¯†ç‚¹</p>
                </div>
'''
        
        html += '''
            </div>
'''
    
    html += '''
        </div>
    </div>
    <script>const { createApp } = Vue; createApp({}).mount('#app')</script>
</body>
</html>'''
    
    return html

def main():
    report_date = sys.argv[2] if len(sys.argv) > 2 else datetime.now().strftime('%Y-%m-%d')
    
    print("=" * 60)
    print(f"ğŸ“° ç”Ÿæˆæ—¶äº‹æŠ¥å‘Š: {report_date}")
    print("=" * 60)
    
    print("\nğŸ“° è·å–æ–°é—»...")
    news_list = fetch_news(report_date)
    print(f"  âœ“ {len(news_list)} æ¡æ–°é—»")
    
    # ä½¿ç”¨å®Œæ•´æ•°æ®åº“
    db_path = 'turso/textbook_full.db'
    
    print("\nğŸ“š åŒ¹é…è¯¾æœ¬çŸ¥è¯†ç‚¹...")
    chapters_dict = {}
    for i, news in enumerate(news_list, 1):
        chapters = search_chapters_full(news, db_path, limit=2)
        chapters_dict[i] = chapters
        if chapters:
            kw = ', '.join(chapters[0].get('matched_keywords', [])[:3])
            book = chapters[0].get('book_name', '')
            chapter = chapters[0].get('chapter_title', '')[:20]
            print(f"  âœ“ æ–°é—»{i}: {book} {chapter} ({kw})")
        else:
            print(f"  â—‹ æ–°é—»{i}: æš‚æ— å…³è”")
    
    print("\nğŸ“„ ç”ŸæˆHTML...")
    html = generate_html(report_date, news_list, chapters_dict)
    
    output_dir = os.path.join(config['paths']['output_dir'], report_date)
    os.makedirs(output_dir, exist_ok=True)
    
    html_path = os.path.join(output_dir, 'index.html')
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html)
    
    # ç”ŸæˆJSON
    json_data = {
        "date": report_date,
        "newsCount": len(news_list),
        "news": [],
        "chapters": []
    }
    
    for i, news in enumerate(news_list, 1):
        json_data["news"].append({
            "rank": i,
            "title": news['title'],
            "source": news['source'],
            "time": news['time'],
            "category": news['category'],
            "summary": news.get('detailed_summary', news['summary']),
            "key_points": news.get('key_points', []),
            "matchedChapters": chapters_dict.get(i, [])
        })
    
    json_path = os.path.join(output_dir, 'report.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œæˆ!")
    print(f"ğŸ“ HTML: {html_path}")
    print(f"ğŸ“ JSON: {json_path}")

if __name__ == "__main__":
    main()
