#!/usr/bin/env python3
"""
æ–°é—»é‡‡é›†ä¸æŠ¥å‘Šç”Ÿæˆå®Œæ•´å·¥ä½œæµ
1. ä»ä¸­å›½æ–°é—»ç½‘è·å–æ ‡é¢˜èŒƒå›´
2. ä¸‹é’»è¯¦æƒ…é¡µè·å–å†…å®¹
3. ç”Ÿæˆé“æ³•æ—¶äº‹æŠ¥å‘Š
"""
import requests
import re
import sqlite3
import json
import time
import os
from datetime import datetime
from bs4 import BeautifulSoup
from tavily import TavilyClient

# é…ç½®
DB_PATH = "/Users/guanliming/dailynews/turso/textbook_full.db"
OUTPUT_DIR = "/Users/guanliming/dailynews/output"
TAVILY_API_KEY = "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"


class NewsCollector:
    """æ–°é—»é‡‡é›†å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        self.tavily = TavilyClient(api_key=TAVILY_API_KEY)
    
    def fetch_list_page(self, url="https://www.chinanews.com.cn/china/"):
        """ä»åˆ—è¡¨é¡µè·å–æ–°é—»æ ‡é¢˜"""
        print(f"ğŸ“° è·å–åˆ—è¡¨: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"  âš ï¸ çŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
            items = soup.select('.hotlist li') or soup.select('.news-list li')
            
            news_list = []
            seen_urls = set()
            
            for item in items[:10]:
                link = item.find('a')
                if not link:
                    continue
                
                title = link.get_text(strip=True)
                href = link.get('href', '')
                
                if not title or len(title) < 10:
                    continue
                
                # æ¸…ç†URL
                if href.startswith('//'):
                    full_url = 'https:' + href
                elif href.startswith('/'):
                    full_url = 'https://www.chinanews.com.cn' + href
                else:
                    full_url = href
                
                if full_url in seen_urls:
                    continue
                seen_urls.add(full_url)
                
                news_list.append({
                    'title': title,
                    'url': full_url,
                    'source': 'ä¸­å›½æ–°é—»ç½‘'
                })
            
            print(f"  âœ“ è·å– {len(news_list)} æ¡æ ‡é¢˜")
            return news_list
            
        except Exception as e:
            print(f"  âš ï¸ è·å–å¤±è´¥: {e}")
            return []
    
    def fetch_detail(self, url, title=""):
        """ä¸‹é’»è·å–è¯¦æƒ…"""
        print(f"  ğŸ” ä¸‹é’»: {title[:40] if title else url}...")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âš ï¸ çŠ¶æ€ç : {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ ‡é¢˜
            if not title:
                title_elem = soup.find('h1')
                title = title_elem.get_text(strip=True) if title_elem else "æ— æ ‡é¢˜"
            
            # æ—¶é—´
            time_str = ""
            time_elem = soup.find('div', class_='pub-time')
            if time_elem:
                time_str = time_elem.get_text(strip=True)
            else:
                date_match = re.search(r'/(\d{4})/(\d{2})-(\d{2})/', url)
                if date_match:
                    time_str = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
            
            # æ¥æº
            source = "ä¸­å›½æ–°é—»ç½‘"
            source_elem = soup.find('div', class_='pub-source')
            if source_elem:
                source = source_elem.get_text(strip=True).replace('æ¥æºï¼š', '')
            
            # æ­£æ–‡
            content = ""
            content_div = soup.find('div', class_='content') or soup.find('article')
            if content_div:
                paragraphs = content_div.find_all('p')
                texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20]
                content = ' '.join(texts)
            
            if not content:
                print(f"    âš ï¸ æ— æ³•æå–æ­£æ–‡")
                return None
            
            # æ‘˜è¦
            summary = content[:400] + ("..." if len(content) > 400 else "")
            
            # åˆ†ç±»
            category = self._guess_category(title)
            
            # å…³é”®è¦ç‚¹
            key_points = self._extract_key_points(content, title)
            
            return {
                'title': title,
                'url': url,
                'source': source,
                'time': time_str[:16] if time_str else datetime.now().strftime('%Y-%m-%d %H:%M'),
                'category': category,
                'content': content,
                'summary': summary,
                'key_points': key_points
            }
            
        except Exception as e:
            print(f"  âš ï¸ é”™è¯¯: {e}")
            return None
    
    def _extract_key_points(self, content, title):
        """æå–å…³é”®è¦ç‚¹"""
        points = []
        
        # äººç‰©
        names = re.findall(r'ä¹ è¿‘å¹³|æå¼º|ä¸è–›ç¥¥|æå¸Œ|ç‹æ¯…|èµµä¹é™…', content)
        if names:
            points.append(f"æ¶‰åŠäººç‰©ï¼š{', '.join(set(names[:2]))}")
        
        # æœºæ„
        orgs = re.findall(r'ä¸­å…±ä¸­å¤®|å›½åŠ¡é™¢|ä¸­å¤®å†›å§”|å›½å°åŠ|å·¥ä¿¡éƒ¨|ç§‘æŠ€éƒ¨|æ•™è‚²éƒ¨', content)
        if orgs:
            points.append(f"æ¶‰åŠæœºæ„ï¼š{', '.join(set(orgs[:2]))}")
        
        # äº‹ä»¶
        events = re.findall(r'(æ•™è‚²æ”¹é©|æœˆçƒæ¢æµ‹|è½½äººèˆªå¤©|åè…è´¥|å»ºå†›|æ°‘ä¸»|æ³•æ²»)', content)
        if events:
            points.append(f"å…³é”®äº‹ä»¶ï¼š{', '.join(set(events[:2]))}")
        
        return points
    
    def _guess_category(self, title):
        """åˆ†ç±»"""
        categories = {
            'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•'],
            'æ³•å¾‹': ['æ³•å¾‹', 'æ³•é™¢', 'æ£€å¯Ÿ', 'å¸æ³•', 'çŠ¯ç½ª'],
            'æ”¿æ²»': ['ä¹ è¿‘å¹³', 'æå¼º', 'ä¼šè®®', 'è®²è¯', 'å…šå»º', 'å†›é˜Ÿ'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'èˆªå¤©', 'æœˆçƒ', 'AI', 'åˆ›æ–°'],
            'ä¸¤å²¸': ['å°æ¹¾', 'ä¸¤å²¸', 'å›½å°åŠ', 'å°æµ·'],
            'å¤–äº¤': ['å¤–äº¤', 'å¤–é•¿', 'è”åˆå›½', 'å›½é™…'],
        }
        
        for cat, kws in categories.items():
            for kw in kws:
                if kw in title:
                    return cat
        return 'æ—¶æ”¿'
    
    def match_chapters(self, news):
        """æ ¹æ®æ–°é—»ç±»å‹æ™ºèƒ½åŒ¹é…è¯¾æœ¬çŸ¥è¯†ç‚¹"""
        title = news.get('title', '')
        content = news.get('content', '')[:500]
        category = news.get('category', '')
        text = title + ' ' + content
        
        # å®šä¹‰åŒ¹é…è§„åˆ™ï¼šå…³é”®è¯ -> ç« èŠ‚ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
        # æ³¨æ„ï¼šæ›´å…·ä½“çš„è§„åˆ™æ”¾åœ¨å‰é¢
        rules = [
            # å°æµ·/ä¸¤å²¸ - é«˜ä¼˜å…ˆçº§
            {
                'keywords': ['å°æ¹¾', 'ä¸¤å²¸', 'å°ç‹¬', 'å›½å°åŠ', 'å°æµ·', 'æ¾æ¹–', 'é«˜é‡‘ç´ æ¢…', 'èµ–æ¸…å¾·'],
                'book': 'ä¹å¹´çº§ä¸Šå†Œ',
                'chapter': 'ä¸­åä¸€å®¶äº²',
                'reason': 'ä¸¤å²¸å…³ç³»ä¸å›½å®¶ç»Ÿä¸€'
            },
            # åè…/è¿æ³• - æœ€é«˜ä¼˜å…ˆçº§ï¼Œå¿…é¡»åœ¨æœ€å‰é¢
            {
                'keywords': ['åè…', 'è¿çºª', 'è¿æ³•', 'å—è´¿', 'å®¡æŸ¥', 'è°ƒæŸ¥', 'è½é©¬', 'ç›‘å§”', 'æ³•æ²»', 'æ£€å¯Ÿé™¢', 'è´ªæ±¡', 'å…¬è¯‰'],
                'book': 'ä¹å¹´çº§ä¸Šå†Œ',
                'chapter': 'æ°‘ä¸»ä¸æ³•æ²»',
                'reason': 'åè…ä¸æ³•æ²»'
            },
            # å›½é˜²/å†›äº‹ - ä»…å½“å†…å®¹ä¸»è¦æ¶‰åŠå†›äº‹æ—¶
            {
                'keywords': ['è§£æ”¾å†›', 'å†›é˜Ÿ', 'å†›äº‹', 'å»ºå†›', 'æˆ˜ç«¯', 'å†›å§”', 'æµ·é©¬æ–¯'],
                'book': 'ä¹å¹´çº§ä¸Šå†Œ',
                'chapter': 'ä¸­åä¸€å®¶äº²',
                'reason': 'å›½é˜²ä¸å›½å®¶å®‰å…¨'
            },
            # ç§‘æŠ€åˆ›æ–°
            {
                'keywords': ['èˆªå¤©', 'æœˆçƒ', 'å«æ˜Ÿ', 'é›ªè±¹', 'æ¢æœˆ', 'è½½äºº', 'é»‘æ´', 'å—æ', 'å¤©å…³'],
                'book': 'ä¹å¹´çº§ä¸Šå†Œ',
                'chapter': 'åˆ›æ–°é©±åŠ¨å‘å±•',
                'reason': 'ç§‘æŠ€åˆ›æ–°'
            },
            # æ•™è‚²
            {
                'keywords': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•', 'æ•™è‚²éƒ¨'],
                'book': 'ä¸ƒå¹´çº§ä¸Šå†Œ',
                'chapter': 'æˆé•¿çš„èŠ‚æ‹',
                'reason': 'æ•™è‚²ä¸å­¦ä¹ '
            },
            # ç¾ä¸½ä¸­å›½/ç¯å¢ƒ
            {
                'keywords': ['ç¯å¢ƒ', 'ç”Ÿæ€', 'ç»¿è‰²', 'ç¢³ä¸­å’Œ'],
                'book': 'ä¹å¹´çº§ä¸Šå†Œ',
                'chapter': 'å»ºè®¾ç¾ä¸½ä¸­å›½',
                'reason': 'ç”Ÿæ€æ–‡æ˜å»ºè®¾'
            },
            # æ°‘ä¸»/äººå¤§
            {
                'keywords': ['æ°‘ä¸»', 'äººå¤§', 'æ”¿å', 'å…¨å›½äººå¤§'],
                'book': 'ä¹å¹´çº§ä¸Šå†Œ',
                'chapter': 'æ°‘ä¸»ä¸æ³•æ²»',
                'reason': 'æ°‘ä¸»åˆ¶åº¦'
            },
            # å¼ºå›½æ¢¦
            {
                'keywords': ['å¼ºå›½', 'å¤å…´', 'æ¢¦æƒ³', 'å°åº·', 'æ°‘æ—å¤å…´'],
                'book': 'ä¹å¹´çº§ä¸Šå†Œ',
                'chapter': 'ä¸­å›½äºº ä¸­å›½æ¢¦',
                'reason': 'ä¸­å›½æ¢¦'
            },
            # ç§‘æŠ€æ”¿ç­–
            {
                'keywords': ['ç§‘æŠ€', 'åˆ›æ–°', 'æœåŠ¡ä¸šæ ‡å‡†'],
                'book': 'ä¹å¹´çº§ä¸Šå†Œ',
                'chapter': 'åˆ›æ–°é©±åŠ¨å‘å±•',
                'reason': 'ç§‘æŠ€åˆ›æ–°æ”¿ç­–'
            },
        ]
        
        # æŒ‰ä¼˜å…ˆçº§åŒ¹é…
        matched = None
        for rule in rules:
            for kw in rule['keywords']:
                if kw in text:
                    matched = rule
                    break
            if matched:
                break
        
        # å¦‚æœæ²¡åŒ¹é…ï¼Œæ ¹æ®åˆ†ç±»å…œåº•
        if not matched:
            category_rules = {
                'ç§‘æŠ€': {'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'åˆ›æ–°é©±åŠ¨å‘å±•', 'reason': 'ç§‘æŠ€ç±»æ–°é—»'},
                'ä¸¤å²¸': {'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'ä¸­åä¸€å®¶äº²', 'reason': 'ä¸¤å²¸å…³ç³»'},
                'æ³•å¾‹': {'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'æ°‘ä¸»ä¸æ³•æ²»', 'reason': 'æ³•å¾‹ç±»'},
                'æ—¶æ”¿': {'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'æ°‘ä¸»ä¸æ³•æ²»', 'reason': 'æ—¶æ”¿è¦é—»'},
            }
            matched = category_rules.get(category, {'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'æ°‘ä¸»ä¸æ³•æ²»', 'reason': 'æ—¶äº‹å…³è”'})
        
        return [{
            'book_name': matched['book'],
            'chapter_title': matched['chapter'],
            'reason': matched['reason']
        }]
    
    def generate_report(self, news_list):
        """ç”ŸæˆæŠ¥å‘Š"""
        report = {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'newsCount': len(news_list),
            'news': []
        }
        
        for i, news in enumerate(news_list, 1):
            # åŒ¹é…è¯¾æœ¬
            matched = self.match_chapters(news)
            
            report['news'].append({
                'rank': i,
                'title': news['title'],
                'source': news['source'],
                'time': news['time'],
                'category': news['category'],
                'summary': news.get('summary', ''),
                'key_points': news.get('key_points', []),
                'url': news['url'],
                'matchedChapters': matched
            })
        
        return report
    
    def run(self):
        """è¿è¡Œ"""
        print("\n" + "="*70)
        print("ğŸ“° é“æ³•æ—¶äº‹æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ")
        print("="*70)
        
        # 1. ä»åˆ—è¡¨é¡µè·å–æ ‡é¢˜
        news_list = self.fetch_list_page()
        
        if not news_list:
            print("  âš ï¸ æ— å‘ç°æ–°é—»")
            return []
        
        # 2. ä¸‹é’»è·å–è¯¦æƒ…
        print("\nğŸ“„ ä¸‹é’»è·å–è¯¦æƒ…...")
        collected = []
        
        for i, news in enumerate(news_list, 1):
            print(f"\n[{i}/{len(news_list)}]")
            detail = self.fetch_detail(news['url'], news['title'])
            
            if detail:
                collected.append(detail)
                print(f"    âœ… {detail['title'][:40]}...")
            time.sleep(0.3)
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(collected)
        
        # 4. ä¿å­˜
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        with open(os.path.join(OUTPUT_DIR, 'report_latest.json'), 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: report_latest.json")
        
        return report


def main():
    collector = NewsCollector()
    report = collector.run()
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“° æŠ¥å‘Šé¢„è§ˆ")
    print("="*70)
    
    for news in report.get('news', []):
        print(f"\n{news['rank']}. {news['title']}")
        print(f"   {news['category']} Â· {news['source']} Â· {news['time']}")
        print(f"   æ‘˜è¦: {news.get('summary', '')[:100]}...")
        if news.get('key_points'):
            for p in news['key_points']:
                print(f"   ğŸ“Œ {p}")


if __name__ == "__main__":
    main()
