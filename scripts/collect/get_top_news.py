#!/usr/bin/env python3
"""
è·å–ä¸­å›½æ–°é—»ç½‘é¡¶éƒ¨ç„¦ç‚¹å›¾ç‰‡æ–°é—»
"""
import requests
import re
import json
import time
from bs4 import BeautifulSoup
from datetime import datetime

# é…ç½®
DB_PATH = "/Users/guanliming/dailynews/turso/textbook_full.db"
OUTPUT_DIR = "/Users/guanliming/dailynews/output"


class ChinaNewsFetcher:
    """ä¸­å›½æ–°é—»ç½‘é¡¶éƒ¨ç„¦ç‚¹æ–°é—»è·å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
    
    def get_top_news(self, max_news=5):
        """è·å–é¡¶éƒ¨ç„¦ç‚¹æ–°é—»"""
        url = "https://www.chinanews.com.cn/china/"
        print(f"ğŸ“° è®¿é—®: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"  âš ï¸ çŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            news_list = []
            seen_urls = set()
            seen_titles = set()
            
            # æ–¹æ³•1: dh6927ab2b3c896d07080e7d8a åŒºåŸŸï¼ˆæ»šåŠ¨æ›´æ–°ï¼ŒåŒ…å«ç”¨æˆ·æä¾›çš„æ­£ç¡®æ ‡é¢˜ï¼‰
            sfq_area = soup.select_one('.dh6927ab2b3c896d07080e7d8a')
            if sfq_area:
                print(f"  âœ… æ‰¾åˆ° dh6927ab2b3c896d07080e7d8a åŒºåŸŸ")
                links = sfq_area.find_all('a')
                
                for a in links:
                    title = a.get_text(strip=True)
                    href = a.get('href', '')
                    
                    # è¡¥å…¨URL
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = 'https://www.chinanews.com.cn' + href
                    
                    # éªŒè¯ï¼šæ ‡é¢˜é•¿åº¦15-80ï¼ŒURLåŒ…å«2026
                    if len(title) < 15 or len(title) > 80:
                        continue
                    if '/2026/' not in href:
                        continue
                    
                    # å»é‡ï¼šæ ‡é¢˜å’ŒURLéƒ½å¯èƒ½æ˜¯é‡å¤çš„
                    title_clean = title[:40]  # å–å‰40å­—ç¬¦å»é‡
                    if title_clean in seen_titles or href in seen_urls:
                        continue
                    seen_titles.add(title_clean)
                    seen_urls.add(href)
                    
                    news_list.append({
                        'title': title,
                        'url': href,
                        'area': 'dh6927ab2b3c896d07080e7d8a'
                    })
            
            # æ–¹æ³•2: news-list åŒºåŸŸ
            if len(news_list) < max_news:
                print(f"  ğŸ” æœç´¢ news-list åŒºåŸŸ...")
                news_links = soup.select('.news-list a')[:max_news * 2]
                
                for a in news_links:
                    title = a.get_text(strip=True)
                    href = a.get('href', '')
                    
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = 'https://www.chinanews.com.cn' + href
                    
                    if len(title) < 15 or '/2026/' not in href:
                        continue
                    
                    title_clean = title[:40]
                    if title_clean in seen_titles or href in seen_urls:
                        continue
                    seen_titles.add(title_clean)
                    seen_urls.add(href)
                    
                    news_list.append({
                        'title': title,
                        'url': href,
                        'area': 'news-list'
                    })
            
            print(f"  ğŸ“Š è·å– {len(news_list)} æ¡é¡¶éƒ¨æ–°é—»")
            return news_list[:max_news]
            
        except Exception as e:
            print(f"  âš ï¸ é”™è¯¯: {e}")
            return []
    
    def fetch_detail(self, news):
        """ä¸‹é’»è·å–è¯¦æƒ…"""
        url = news['url']
        print(f"  ğŸ” ä¸‹é’»: {news['title'][:40]}...")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âš ï¸ çŠ¶æ€ç : {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ ‡é¢˜
            title_elem = soup.find('h1')
            if title_elem:
                news['title'] = title_elem.get_text(strip=True)
            
            # æ—¶é—´
            time_elem = soup.find('div', class_='pub-time')
            if time_elem:
                news['time'] = time_elem.get_text(strip=True)
            else:
                date_match = re.search(r'/(\d{4})/(\d{2})-(\d{2})/', url)
                if date_match:
                    news['time'] = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
            
            # æ¥æº
            source_elem = soup.find('div', class_='pub-source')
            if source_elem:
                news['source'] = source_elem.get_text(strip=True).replace('æ¥æºï¼š', '')
            else:
                news['source'] = 'ä¸­å›½æ–°é—»ç½‘'
            
            # æ­£æ–‡
            content = ""
            content_div = soup.find('div', class_='content') or soup.find('article')
            if content_div:
                paragraphs = content_div.find_all('p')
                texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20]
                content = ' '.join(texts)
            
            if not content:
                print(f"    âš ï¸ æ— æ³•æå–æ­£æ–‡")
                return news
            
            # æ‘˜è¦
            news['content'] = content
            news['summary'] = content[:300] + ("..." if len(content) > 300 else "")
            
            # åˆ†ç±»
            news['category'] = self._guess_category(news['title'])
            
            # å…³é”®è¦ç‚¹
            news['key_points'] = self._extract_key_points(content)
            
            print(f"    âœ… æˆåŠŸ")
            return news
            
        except Exception as e:
            print(f"    âš ï¸ é”™è¯¯: {e}")
            return news
    
    def _extract_key_points(self, content):
        """æå–å…³é”®è¦ç‚¹"""
        points = []
        
        names = re.findall(r'ä¹ è¿‘å¹³|æå¼º|ä¸è–›ç¥¥|æå¸Œ|ç‹æ¯…|èµµä¹é™…', content)
        if names:
            points.append(f"æ¶‰åŠäººç‰©ï¼š{', '.join(set(names[:2]))}")
        
        orgs = re.findall(r'ä¸­å…±ä¸­å¤®|å›½åŠ¡é™¢|ä¸­å¤®å†›å§”|å›½å°åŠ|å·¥ä¿¡éƒ¨|ç§‘æŠ€éƒ¨|æ•™è‚²éƒ¨', content)
        if orgs:
            points.append(f"æ¶‰åŠæœºæ„ï¼š{', '.join(set(orgs[:2]))}")
        
        events = re.findall(r'(æ•™è‚²æ”¹é©|æœˆçƒæ¢æµ‹|è½½äººèˆªå¤©|åè…è´¥|å»ºå†›|æ°‘ä¸»|æ³•æ²»)', content)
        if events:
            points.append(f"å…³é”®äº‹ä»¶ï¼š{', '.join(set(events[:2]))}")
        
        return points
    
    def _guess_category(self, title):
        """åˆ†ç±»"""
        categories = {
            'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ'],
            'æ³•å¾‹': ['æ³•å¾‹', 'æ³•é™¢', 'æ£€å¯Ÿ', 'å¸æ³•'],
            'æ”¿æ²»': ['ä¹ è¿‘å¹³', 'æå¼º', 'ä¼šè®®', 'è®²è¯', 'å…šå»º', 'å†›é˜Ÿ'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'èˆªå¤©', 'æœˆçƒ', 'AI'],
            'ä¸¤å²¸': ['å°æ¹¾', 'ä¸¤å²¸', 'å°æµ·'],
        }
        
        for cat, kws in categories.items():
            for kw in kws:
                if kw in title:
                    return cat
        return 'æ—¶æ”¿'
    
    def run(self, max_news=5):
        """è¿è¡Œ"""
        print("\n" + "="*60)
        print("ğŸ“° ä¸­å›½æ–°é—»ç½‘é¡¶éƒ¨ç„¦ç‚¹æ–°é—»")
        print("="*60)
        
        # 1. è·å–é¡¶éƒ¨æ–°é—»æ ‡é¢˜
        news_list = self.get_top_news(max_news)
        
        if not news_list:
            print("  âš ï¸ æ— æ–°é—»")
            return []
        
        # 2. ä¸‹é’»è·å–è¯¦æƒ…
        print("\nğŸ” ä¸‹é’»è·å–è¯¦æƒ…...")
        results = []
        
        for i, news in enumerate(news_list, 1):
            print(f"\n[{i}/{len(news_list)}]")
            detail = self.fetch_detail(news)
            if detail:
                results.append(detail)
            time.sleep(0.3)
        
        return results


def main():
    fetcher = ChinaNewsFetcher()
    results = fetcher.run(max_news=5)
    
    # ä¿å­˜
    import os
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    report = {
        'date': datetime.now().strftime('%Y-%m-%d'),
        'count': len(results),
        'news': results
    }
    
    with open(f'{OUTPUT_DIR}/top_news.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å·²ä¿å­˜: {OUTPUT_DIR}/top_news.json")
    
    # æ‰“å°
    for i, n in enumerate(results, 1):
        print(f"\n{i}. {n['title'][:50]}")
        print(f"   {n.get('source', 'ä¸­å›½æ–°é—»ç½‘')} | {n.get('time', '')}")


if __name__ == "__main__":
    main()
