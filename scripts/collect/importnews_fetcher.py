#!/usr/bin/env python3
"""
ä¸­æ–°çƒ­æ¦œæ–°é—»é‡‡é›†å™¨
ä» https://www.chinanews.com.cn/importnews.html è·å–çƒ­æ¦œæ–°é—»
å­˜å‚¨åˆ° Turso æ•°æ®åº“
"""
import requests
import re
import sqlite3
import json
import time
from datetime import datetime
from bs4 import BeautifulSoup

# é…ç½®
DB_PATH = "/Users/guanliming/dailynews/turso/reports.db"
OUTPUT_DIR = "/Users/guanliming/dailynews/output"
BASE_URL = "https://www.chinanews.com.cn"


class ImportNewsFetcher:
    """ä¸­æ–°çƒ­æ¦œæ–°é—»é‡‡é›†å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
    
    def fetch_hot_list(self, url=f"{BASE_URL}/importnews.html"):
        """è·å–çƒ­æ¦œæ–°é—»åˆ—è¡¨"""
        print(f"\nğŸ“° è·å–çƒ­æ¦œ: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"  âš ï¸ çŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ - æ³¨æ„ç»“æ„æ˜¯ .content_list > ul > li
            container = soup.find('div', class_='content_list')
            if not container:
                print(f"  âš ï¸ æœªæ‰¾åˆ°æ–°é—»åˆ—è¡¨å®¹å™¨")
                return []
            
            items = container.find_all('li')
            
            news_list = []
            seen_urls = set()
            today = datetime.now().strftime('%Y-%m-%d')
            
            for item in items:
                # è·³è¿‡ç©ºåˆ†éš”ç¬¦ - class å¯èƒ½æ˜¯ nocontent æˆ– class_='nocontent'
                if 'nocontent' in item.get('class', []):
                    continue
                
                # è·å–æ ‡é¢˜å’Œé“¾æ¥ - dd_bt å†…éƒ¨å¯èƒ½æœ‰å¤šä¸ª a
                title_elem = item.find('div', class_='dd_bt')
                if not title_elem:
                    continue
                
                links = title_elem.find_all('a')
                if not links:
                    continue
                
                # è·å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„é“¾æ¥
                for link in links:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    
                    # è·³è¿‡ iframe å’Œè§†é¢‘é“¾æ¥
                    if '/iframe/' in href or '/shipin/' in href:
                        continue
                    if not title or len(title) < 5:
                        continue
                    
                    # æ¸…ç†URL
                    if href.startswith('//'):
                        full_url = 'https:' + href
                    elif href.startswith('/'):
                        full_url = BASE_URL + href
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue
                    
                    if full_url in seen_urls:
                        continue
                    seen_urls.add(full_url)
                    
                    # è·å–æ—¶é—´
                    time_elem = item.find('div', class_='dd_time')
                    time_str = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # è§£ææ—¥æœŸ
                    pub_date = self._parse_date(time_str, today)
                    
                    # è·å–é¢‘é“
                    channel_elem = item.find('div', class_='dd_lm')
                    channel = channel_elem.get_text(strip=True).strip('[]') if channel_elem else "è¦é—»"
                    
                    news_list.append({
                        'title': title,
                        'url': full_url,
                        'source': 'ä¸­å›½æ–°é—»ç½‘',
                        'channel': channel,
                        'time': time_str,
                        'publish_date': pub_date
                    })
                    break  # åªå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„é“¾æ¥
            
            print(f"  âœ“ è·å– {len(news_list)} æ¡çƒ­æ¦œæ–°é—»")
            return news_list
            
        except Exception as e:
            print(f"  âš ï¸ è·å–å¤±è´¥: {e}")
            return []
    
    def _parse_date(self, time_str, default_date):
        """è§£ææ—¥æœŸ"""
        if not time_str:
            return default_date
        
        # æ ¼å¼: "2-12 13:27" -> "2026-02-12"
        try:
            month, day = time_str.split()[0].split('-')
            year = datetime.now().year
            return f"{year}-{int(month):02d}-{int(day):02d}"
        except:
            return default_date
    
    def fetch_detail(self, url, title=""):
        """è·å–æ–°é—»è¯¦æƒ…"""
        print(f"  ğŸ” ä¸‹é’»: {title[:30] if title else url}...")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âš ï¸ çŠ¶æ€ç : {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ ‡é¢˜
            if not title:
                title_elem = soup.find('h1')
                title = title_elem.get_text(strip=True) if title_elem else "æ— æ ‡é¢˜"
            
            # æ—¶é—´
            time_str = ""
            time_elem = soup.find('div', class_='pub-time')
            if time_elem:
                time_str = time_elem.get_text(strip=True)
            else:
                date_match = re.search(r'/(\d{4})/(\d{2})-(\d{2})/', url)
                if date_match:
                    time_str = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
            
            # æ¥æº
            source = "ä¸­å›½æ–°é—»ç½‘"
            source_elem = soup.find('div', class_='pub-source')
            if source_elem:
                source = source_elem.get_text(strip=True).replace('æ¥æºï¼š', '')
            
            # æ­£æ–‡
            content = ""
            content_div = soup.find('div', class_='content') or soup.find('article')
            if content_div:
                paragraphs = content_div.find_all('p')
                texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20]
                content = ' '.join(texts)
            
            if not content:
                # å°è¯•å…¶ä»–é€‰æ‹©å™¨
                content_div = soup.find('div', id='content_text') or soup.find('div', class_='article')
                if content_div:
                    paragraphs = content_div.find_all('p')
                    texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20]
                    content = ' '.join(texts)
            
            if not content:
                print(f"    âš ï¸ æ— æ³•æå–æ­£æ–‡")
                return None
            
            # æ‘˜è¦
            summary = content[:400] + ("..." if len(content) > 400 else "")
            
            # åˆ†ç±»
            category = self._guess_category(title)
            
            return {
                'title': title,
                'url': url,
                'source': source,
                'time': time_str[:16] if time_str else datetime.now().strftime('%Y-%m-%d %H:%M'),
                'category': category,
                'content': content,
                'summary': summary,
            }
            
        except Exception as e:
            print(f"  âš ï¸ é”™è¯¯: {e}")
            return None
    
    def _guess_category(self, title):
        """åˆ†ç±»"""
        categories = {
            'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•'],
            'æ³•å¾‹': ['æ³•é™¢', 'æ£€å¯Ÿ', 'å¸æ³•', 'çŠ¯ç½ª'],
            'æ”¿æ²»': ['ä¹ è¿‘å¹³', 'æå¼º', 'ä¼šè®®', 'è®²è¯', 'å…šå»º'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'èˆªå¤©', 'æœˆçƒ', 'AI', 'åˆ›æ–°'],
            'ä¸¤å²¸': ['å°æ¹¾', 'ä¸¤å²¸', 'å›½å°åŠ', 'å°æµ·'],
            'å›½é™…': ['ç¾å›½', 'æ—¥æœ¬', 'éŸ©å›½', 'è”åˆå›½', 'å›½é™…'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'äº¤é€š', 'ç¯å¢ƒ'],
            'ç»æµ': ['ç»æµ', 'é‡‘è', 'ä¼ä¸š', 'å¸‚åœº'],
        }
        
        for cat, kws in categories.items():
            for kw in kws:
                if kw in title:
                    return cat
        return 'è¦é—»'
    
    def save_to_db(self, news_list):
        """ä¿å­˜åˆ° Turso æ•°æ®åº“"""
        print(f"\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“: {DB_PATH}")
        
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆä¿ç•™è¡¨ç»“æ„ï¼‰
        print("  ğŸ—‘ï¸ æ¸…ç©ºç°æœ‰æ•°æ®...")
        cursor.execute("DELETE FROM news_articles")
        cursor.execute("DELETE FROM daily_reports")
        cursor.execute("DELETE FROM report_chapter_mapping")
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        date_groups = {}
        for news in news_list:
            date = news.get('publish_date', datetime.now().strftime('%Y-%m-%d'))
            if date not in date_groups:
                date_groups[date] = []
            date_groups[date].append(news)
        
        # ä¿å­˜æ–°é—»å¹¶ç”ŸæˆæŠ¥å‘Š
        report_count = 0
        for date, items in sorted(date_groups.items(), reverse=True):
            report_id = f"{date}_0"
            
            # ä¿å­˜åˆ° news_articles
            for news in items:
                cursor.execute("""
                    INSERT INTO news_articles 
                    (id, title, url, source, publish_date, content, summary, category, channel)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    f"{date}_{items.index(news)}",
                    news['title'],
                    news['url'],
                    news.get('source', 'ä¸­å›½æ–°é—»ç½‘'),
                    news.get('publish_date', date),
                    news.get('content', ''),
                    news.get('summary', ''),
                    news.get('category', 'è¦é—»'),
                    news.get('channel', 'è¦é—»')
                ))
            
            # ä¿å­˜æŠ¥å‘Šæ±‡æ€»
            cursor.execute("""
                INSERT INTO daily_reports 
                (id, report_date, news_count, report_html)
                VALUES (?, ?, ?, ?)
            """, (
                report_id,
                date,
                len(items),
                f"{date}/index.html"
            ))
            
            report_count += 1
            print(f"  âœ“ {date}: {len(items)} æ¡æ–°é—»")
        
        conn.commit()
        conn.close()
        
        print(f"\nâœ… å®Œæˆï¼å…±ä¿å­˜ {len(news_list)} æ¡æ–°é—»ï¼Œ{report_count} ä»½æŠ¥å‘Š")
        return True
    
    def run(self):
        """è¿è¡Œé‡‡é›†"""
        print("\n" + "="*70)
        print("ğŸ“° ä¸­æ–°çƒ­æ¦œæ–°é—»é‡‡é›†å™¨")
        print("="*70)
        
        # 1. è·å–çƒ­æ¦œåˆ—è¡¨
        news_list = self.fetch_hot_list()
        
        if not news_list:
            print("  âš ï¸ æ— å‘ç°æ–°é—»")
            return []
        
        # 2. ä¸‹é’»è·å–è¯¦æƒ…
        print("\nğŸ“„ ä¸‹é’»è·å–è¯¦æƒ…...")
        collected = []
        
        for i, news in enumerate(news_list, 1):
            print(f"\n[{i}/{len(news_list)}]")
            detail = self.fetch_detail(news['url'], news['title'])
            
            if detail:
                # åˆå¹¶åŸºæœ¬ä¿¡æ¯
                detail['channel'] = news.get('channel', 'è¦é—»')
                detail['publish_date'] = news.get('publish_date', datetime.now().strftime('%Y-%m-%d'))
                collected.append(detail)
                print(f"    âœ… {detail['title'][:40]}...")
            time.sleep(0.2)
        
        # 3. ä¿å­˜åˆ°æ•°æ®åº“
        if collected:
            self.save_to_db(collected)
        
        return collected


def main():
    fetcher = ImportNewsFetcher()
    fetcher.run()


if __name__ == "__main__":
    main()
