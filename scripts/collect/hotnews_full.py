#!/usr/bin/env python3
"""
ä¸­æ–°çƒ­æ¦œå®Œæ•´é‡‡é›†å™¨
1. ä» importnews.html è·å–æ‰€æœ‰æ–°é—»æ ‡é¢˜å’Œé“¾æ¥
2. æ‰¹é‡è·å–æ¯æ¡æ–°é—»çš„è¯¦ç»†å†…å®¹
3. ä¿å­˜åˆ°æ•°æ®åº“
"""
import requests
import sqlite3
import json
import time
from datetime import datetime
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®
DB_PATH = "/Users/guanliming/dailynews/turso/reports.db"
OUTPUT_DIR = "/Users/guanliming/dailynews/output"
BASE_URL = "https://www.chinanews.com.cn"
OUTPUT_FILE = f"{OUTPUT_DIR}/hotnews_detail.json"

def get_hot_list():
    """è·å–çƒ­æ¦œæ‰€æœ‰æ–°é—»"""
    print("ğŸ“° è·å–ä¸­æ–°çƒ­æ¦œ...")
    response = requests.get(f"{BASE_URL}/importnews.html", timeout=15)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, 'html.parser')
    
    container = soup.find('div', class_='content_list')
    if not container:
        print("  âš ï¸ æœªæ‰¾åˆ°åˆ—è¡¨")
        return []
    
    items = container.find_all('li')
    news_list = []
    seen_urls = set()
    
    for item in items:
        if 'nocontent' in item.get('class', []):
            continue
        
        title_elem = item.find('div', class_='dd_bt')
        if not title_elem:
            continue
        
        links = title_elem.find_all('a')
        for link in links:
            href = link.get('href', '')
            title = link.get_text(strip=True)
            
            if '/iframe/' in href or '/shipin/' in href or not title or len(title) < 5:
                continue
            
            if href.startswith('//'):
                full_url = 'https:' + href
            elif href.startswith('/'):
                full_url = BASE_URL + href
            else:
                continue
            
            if full_url in seen_urls:
                continue
            seen_urls.add(full_url)
            
            time_elem = item.find('div', class_='dd_time')
            time_str = time_elem.get_text(strip=True) if time_elem else ""
            
            try:
                parts = time_str.split()[0].split('-')
                month, day = int(parts[0]), int(parts[1])
                pub_date = f"{datetime.now().year}-{month:02d}-{day:02d}"
            except:
                pub_date = datetime.now().strftime('%Y-%m-%d')
            
            channel_elem = item.find('div', class_='dd_lm')
            channel = channel_elem.get_text(strip=True).strip('[]') if channel_elem else "è¦é—»"
            
            news_list.append({
                'title': title,
                'url': full_url,
                'source': 'ä¸­å›½æ–°é—»ç½‘',
                'channel': channel,
                'time': time_str,
                'publish_date': pub_date
            })
            break
    
    print(f"  âœ… è·å– {len(news_list)} æ¡æ–°é—»")
    return news_list

def get_content(url, title):
    """è·å–å•æ¡æ–°é—»å†…å®¹"""
    try:
        resp = requests.get(url, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # è·å–æ­£æ–‡
        content = ""
        content_div = soup.find('div', class_='content')
        if content_div:
            ps = content_div.find_all('p')
            texts = [p.get_text(strip=True) for p in ps if len(p.get_text(strip=True)) > 20]
            content = ' '.join(texts)
        
        # æ‘˜è¦
        summary = content[:300] + ("..." if len(content) > 300 else "")
        
        # åˆ†ç±»
        category = guess_category(title + " " + content[:500])
        
        return {
            'title': title,
            'url': url,
            'content': content,
            'summary': summary,
            'category': category,
            'status': 'success'
        }
    except Exception as e:
        return {
            'title': title,
            'url': url,
            'error': str(e),
            'status': 'failed'
        }

def guess_category(text):
    """æ ¹æ®å†…å®¹åˆ†ç±»"""
    categories = {
        'ç§‘æŠ€': ['ç§‘æŠ€', 'AI', 'å¾®ä¿¡', 'äº’è”ç½‘', 'æ•°å­—'],
        'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'è€ƒè¯•'],
        'æ³•å¾‹': ['æ³•é™¢', 'æ£€å¯Ÿ', 'å¸æ³•', 'è¿æ³•', 'çŠ¯ç½ª'],
        'ä¸¤å²¸': ['å°æ¹¾', 'ä¸¤å²¸', 'å›½å°åŠ', 'å°æµ·', 'èµ–æ¸…å¾·'],
        'å›½é™…': ['ç¾å›½', 'æ—¥æœ¬', 'éŸ©å›½', 'å›½é™…', 'å¤–åª’'],
        'ç»æµ': ['ç»æµ', 'é‡‘ä»·', 'å°±ä¸š', 'å…³ç¨', 'ä¼ä¸š'],
        'ç¤¾ä¼š': ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'äº¤é€š', 'ç”Ÿæ´»'],
    }
    
    for cat, kws in categories.items():
        for kw in kws:
            if kw in text:
                return cat
    return 'è¦é—»'

def match_chapter(news):
    """åŒ¹é…é“æ³•è¯¾æœ¬ç« èŠ‚"""
    title = news.get('title', '')
    content = news.get('content', '')[:500]
    category = news.get('category', '')
    text = title + ' ' + content
    
    rules = [
        {'kws': ['å°æ¹¾', 'ä¸¤å²¸', 'å°ç‹¬', 'å›½å°åŠ', 'å°æµ·', 'èµ–æ¸…å¾·'], 'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'ä¸­åä¸€å®¶äº²'},
        {'kws': ['åè…', 'è¿çºª', 'è¿æ³•', 'å—è´¿', 'è°ƒæŸ¥', 'æ£€å¯Ÿé™¢'], 'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'æ°‘ä¸»ä¸æ³•æ²»'},
        {'kws': ['å›½é˜²', 'è§£æ”¾å†›', 'å†›é˜Ÿ', 'å†›äº‹'], 'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'ä¸­åä¸€å®¶äº²'},
        {'kws': ['èˆªå¤©', 'æœˆçƒ', 'å«æ˜Ÿ', 'ç§‘æŠ€', 'åˆ›æ–°', 'åŒ»ç–—'], 'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'åˆ›æ–°é©±åŠ¨å‘å±•'},
        {'kws': ['ç¾å›½', 'æ—¥æœ¬', 'éŸ©å›½', 'å›½é™…'], 'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'å»ºè®¾ç¾ä¸½ä¸­å›½'},
        {'kws': ['å°±ä¸š', 'ç»æµ', 'å…³ç¨', 'ä¼ä¸š'], 'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'å¯Œå¼ºä¸åˆ›æ–°'},
        {'kws': ['ç”Ÿæ´»', 'æ°‘ç”Ÿ'], 'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'å»ºè®¾ç¾ä¸½ä¸­å›½'},
    ]
    
    for rule in rules:
        for kw in rule['kws']:
            if kw in text:
                return {'book': rule['book'], 'chapter': rule['chapter'], 'reason': 'å…³é”®è¯åŒ¹é…'}
    
    return {'book': 'ä¹å¹´çº§ä¸Šå†Œ', 'chapter': 'æ°‘ä¸»ä¸æ³•æ²»', 'reason': 'é»˜è®¤åˆ†ç±»'}

def save_to_db(all_news):
    """ä¿å­˜åˆ°æ•°æ®åº“"""
    print(f"\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # æ¸…ç©ºç°æœ‰æ•°æ®
    cursor.execute("DELETE FROM daily_reports")
    cursor.execute("DELETE FROM report_chapter_mapping")
    
    # è·å–æœ‰æ•ˆæ–°é—»ï¼ˆæˆåŠŸçš„ï¼‰
    valid_news = [n for n in all_news if n.get('status') == 'success']
    print(f"  ğŸ“Š æœ‰æ•ˆæ–°é—»: {len(valid_news)} æ¡")
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    date_groups = {}
    for news in valid_news:
        date = news.get('publish_date', datetime.now().strftime('%Y-%m-%d'))
        date_groups.setdefault(date, []).append(news)
    
    # ä¿å­˜
    total = 0
    for date in sorted(date_groups.keys(), reverse=True):
        items = date_groups[date]
        for idx, news in enumerate(items, 1):
            chapter = match_chapter(news)
            
            # ä¿å­˜åˆ° daily_reports
            cursor.execute("""
                INSERT INTO daily_reports 
                (id, report_date, news_rank, news_title, source, publish_time, summary)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                f"{date}_{idx}",
                date,
                idx,
                news['title'],
                news.get('source', 'ä¸­å›½æ–°é—»ç½‘'),
                news.get('time', date),
                news.get('summary', '')[:500]
            ))
            
            total += 1
    
    conn.commit()
    conn.close()
    print(f"  âœ… ä¿å­˜å®Œæˆï¼å…± {total} æ¡")
    return total

def main():
    print("="*60)
    print("ğŸ“° ä¸­æ–°çƒ­æ¦œå®Œæ•´é‡‡é›†å™¨")
    print("="*60)
    
    # 1. è·å–çƒ­æ¦œåˆ—è¡¨
    news_list = get_hot_list()
    if not news_list:
        print("  âš ï¸ æ— æ–°é—»")
        return
    
    # 2. æ‰¹é‡è·å–è¯¦æƒ…ï¼ˆä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿï¼‰
    print(f"\nğŸ“„ è·å– {len(news_list)} æ¡æ–°é—»è¯¦æƒ…...")
    all_results = []
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(get_content, n['url'], n['title']): n for n in news_list}
        for i, future in enumerate(as_completed(futures), 1):
            result = future.result()
            result['publish_date'] = news_list[i-1].get('publish_date', '')
            result['time'] = news_list[i-1].get('time', '')
            result['channel'] = news_list[i-1].get('channel', '')
            all_results.append(result)
            print(f"  [{i}/{len(news_list)}] {result['title'][:30]}... [{result.get('status', 'unknown')}]")
    
    # 3. ä¿å­˜åˆ°æ•°æ®åº“
    save_to_db(all_results)
    
    # 4. ä¿å­˜JSON
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… JSONå·²ä¿å­˜: {OUTPUT_FILE}")
    
    # 5. ç»Ÿè®¡
    success = sum(1 for n in all_results if n.get('status') == 'success')
    print(f"\nğŸ“Š å®Œæˆï¼æˆåŠŸ {success}/{len(news_list)} æ¡")

if __name__ == "__main__":
    main()
