#!/usr/bin/env python3
"""
æ–°é—»é‡‡é›†å®Œæ•´å·¥ä½œæµï¼ˆæ··åˆæ–¹æ¡ˆï¼‰
1. Tavily å‘ç°æ–°é—»é“¾æ¥ â†’ 2. ä¸‹é’»è¯¦æƒ…é¡µè·å–å†…å®¹ â†’ 3. å­˜å‚¨åˆ° Turso â†’ 4. å‘é‡åŒ–
"""
import requests
import re
import sqlite3
import json
import time
import os
from datetime import datetime
from bs4 import BeautifulSoup
from tavily import TavilyClient

# é…ç½®
DB_PATH = "/Users/guanliming/dailynews/turso/textbook_full.db"
OUTPUT_DIR = "/Users/guanliming/dailynews/output"
TAVILY_API_KEY = "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"


class NewsCollector:
    """æ–°é—»é‡‡é›†å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        self.tavily = TavilyClient(api_key=TAVILY_API_KEY)
    
    def discover_via_tavily(self, query="2026å¹´2æœˆ ä¸­å›½æ–°é—»ç½‘ æ—¶æ”¿æ–°é—»", max_results=10):
        """é€šè¿‡ Tavily å‘ç°æ–°é—»é“¾æ¥"""
        print(f"\nğŸ” Tavily æœç´¢: {query}")
        
        try:
            response = self.tavily.search(
                query=query,
                max_results=max_results,
                include_answer=False
            )
            
            results = response.get('results', [])
            print(f"  âœ“ è·å– {len(results)} æ¡ç»“æœ")
            
            news_list = []
            for r in results:
                url = r.get('url', '')
                title = r.get('title', '')
                
                # åªä¿ç•™ä¸­å›½æ–°é—»ç½‘é“¾æ¥
                if 'chinanews.com.cn' in url and '/2026/' in url:
                    news_list.append({
                        'title': title,
                        'url': url,
                        'source': 'ä¸­å›½æ–°é—»ç½‘'
                    })
            
            print(f"  âœ“ ç­›é€‰å‡º {len(news_list)} æ¡æ–°é—»ç½‘é“¾æ¥")
            return news_list
            
        except Exception as e:
            print(f"  âš ï¸ Tavily æœç´¢å¤±è´¥: {e}")
            return []
    
    def fetch_detail_page(self, url, search_title=""):
        """ä¸‹é’»åˆ°è¯¦æƒ…é¡µè·å–å†…å®¹"""
        title = search_title
        print(f"  ğŸ” ä¸‹é’»: {url[-50:]}")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    âš ï¸ çŠ¶æ€ç : {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_elem = soup.find('h1')
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            # æå–æ—¶é—´
            time_str = ""
            time_elem = soup.find('div', class_='pub-time')
            if time_elem:
                time_str = time_elem.get_text(strip=True)
            else:
                date_match = re.search(r'/(\d{4})/(\d{2})-(\d{2})/', url)
                if date_match:
                    time_str = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
            
            # æå–æ¥æº
            source = "ä¸­å›½æ–°é—»ç½‘"
            source_elem = soup.find('div', class_='pub-source')
            if source_elem:
                source = source_elem.get_text(strip=True).replace('æ¥æºï¼š', '')
            
            # æå–æ­£æ–‡
            content = ""
            content_div = soup.find('div', class_='content') or \
                         soup.find('article') or \
                         soup.find('div', class_='article')
            
            if content_div:
                paragraphs = content_div.find_all('p')
                texts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if len(text) > 20:
                        texts.append(text)
                content = ' '.join(texts)
            
            if not content:
                print(f"    âš ï¸ æ— æ³•æå–æ­£æ–‡")
                return None
            
            # ç”Ÿæˆæ‘˜è¦
            summary = content[:300] + ("..." if len(content) > 300 else "")
            
            # æå–å…³é”®è¦ç‚¹
            key_points = self._extract_key_points(content)
            
            return {
                'title': title,
                'url': url,
                'source': source,
                'publish_date': time_str[:10] if len(time_str) >= 10 else datetime.now().strftime('%Y-%m-%d'),
                'content': content,
                'summary': summary,
                'key_points': key_points,
                'category': self._guess_category(title),
            }
            
        except Exception as e:
            print(f"  âš ï¸ è¯¦æƒ…è·å–å¤±è´¥: {e}")
            return None
    
    def _extract_key_points(self, content):
        """æå–å…³é”®è¦ç‚¹"""
        key_points = []
        
        names = re.findall(r'ä¹ è¿‘å¹³|æå¼º|ä¸è–›ç¥¥|æå¸Œ|ç‹æ¯…|èµµä¹é™…', content)
        if names:
            key_points.append(f"äººç‰©ï¼š{', '.join(set(names))}")
        
        orgs = re.findall(r'ä¸­å…±ä¸­å¤®|å›½åŠ¡é™¢|ä¸­å¤®å†›å§”|å›½å°åŠ|å·¥ä¿¡éƒ¨|ç§‘æŠ€éƒ¨|æ•™è‚²éƒ¨|å›½å®¶èˆªå¤©å±€', content)
        if orgs:
            key_points.append(f"æœºæ„ï¼š{', '.join(set(orgs))}")
        
        events = re.findall(r'(æœˆçƒæ¢æµ‹|è½½äººèˆªå¤©|åè…è´¥|å»ºå†›|æ”¹é©|ç ”åˆ¶|çªç ´|æ°‘ä¸»|æ³•æ²»)', content)
        if events:
            key_points.append(f"å…³é”®è¯ï¼š{', '.join(set(events))}")
        
        return key_points
    
    def _guess_category(self, title):
        """çŒœæµ‹åˆ†ç±»"""
        categories = {
            'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•'],
            'æ³•å¾‹': ['æ³•å¾‹', 'æ³•é™¢', 'æ£€å¯Ÿ', 'å¸æ³•', 'çŠ¯ç½ª'],
            'ç»æµ': ['ç»æµ', 'GDP', 'CPI', 'ç»Ÿè®¡å±€', 'å¸‚åœº'],
            'æ”¿æ²»': ['ä¹ è¿‘å¹³', 'æå¼º', 'ä¼šè®®', 'è®²è¯', 'å…šå»º', 'å†›é˜Ÿ', 'ä¸­å…±ä¸­å¤®'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'èˆªå¤©', 'æœˆçƒ', 'AI', 'åˆ›æ–°'],
            'ä¸¤å²¸': ['å°æ¹¾', 'ä¸¤å²¸', 'å›½å°åŠ', 'å°æµ·'],
        }
        
        for cat, keywords in categories.items():
            for kw in keywords:
                if kw in title:
                    return cat
        return 'æ—¶æ”¿'
    
    def save_to_db(self, news):
        """ä¿å­˜åˆ° SQLite"""
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT OR REPLACE INTO news_articles 
                (title, url, source, publish_date, content, summary, category, metadata, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                news['title'],
                news['url'],
                news['source'],
                news['publish_date'],
                news['content'],
                news['summary'],
                news['category'],
                json.dumps({'key_points': news.get('key_points', [])}, ensure_ascii=False),
                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            ))
            conn.commit()
            print(f"    ğŸ’¾ å·²ä¿å­˜åˆ°æ•°æ®åº“")
            return True
        except Exception as e:
            print(f"    âš ï¸ æ•°æ®åº“é”™è¯¯: {e}")
            return False
        finally:
            conn.close()
    
    def save_to_json(self, news, index=1):
        """ä¿å­˜åˆ° JSON"""
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        vector = self.generate_vector(news.get('content', ''))
        
        output = {
            **news,
            'vector': vector,
            'word_count': len(news.get('content', '')),
        }
        
        with open(os.path.join(OUTPUT_DIR, f'news_{index}.json'), 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"    ğŸ“ å·²ä¿å­˜")
    
    def generate_vector(self, text):
        """ç”Ÿæˆå…³é”®è¯å‘é‡"""
        words = re.findall(r'[\u4e00-\u9fff]{2,}', text)
        freq = {}
        for w in words:
            freq[w] = freq.get(w, 0) + 1
        return dict(sorted(freq.items(), key=lambda x: x[1], reverse=True)[:20])
    
    def run(self, target_title=None, max_news=5):
        """è¿è¡Œé‡‡é›†æµç¨‹"""
        print("\n" + "="*60)
        print("ğŸ“° æ–°é—»é‡‡é›†å·¥ä½œæµ")
        print("="*60)
        
        # 1. å‘ç°æ–°é—»
        if target_title:
            news_list = self.discover_via_tavily(f"{target_title} site:chinanews.com.cn 2026", max_results=max_news)
        else:
            news_list = self.discover_via_tavily("2026å¹´2æœˆ ä¸­å›½æ—¶æ”¿æ–°é—»", max_results=max_news)
        
        if not news_list:
            print("  âš ï¸ æ— å‘ç°æ–°é—»")
            return []
        
        # 2. å»é‡
        seen = set()
        unique_list = []
        for n in news_list:
            if n['url'] not in seen:
                seen.add(n['url'])
                unique_list.append(n)
        
        # 3. é‡‡é›†è¯¦æƒ…
        collected = []
        for i, news in enumerate(unique_list[:max_news], 1):
            print(f"\n[{i}/{len(unique_list[:max_news])}]")
            detail = self.fetch_detail_page(news['url'], news['title'])
            
            if detail:
                self.save_to_db(detail)
                self.save_to_json(detail, index=i)
                collected.append(detail)
                print(f"    âœ… {detail['title'][:30]}...")
            time.sleep(0.3)
        
        print(f"\n" + "="*60)
        print(f"âœ… é‡‡é›†å®Œæˆ! å…± {len(collected)} æ¡")
        print("="*60)
        
        return collected


def main():
    """ä¸»å…¥å£"""
    import sys
    
    target = sys.argv[1] if len(sys.argv) > 1 else None
    max_news = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    collector = NewsCollector()
    results = collector.run(target_title=target, max_news=max_news)
    
    # æ‰“å°æ‘˜è¦
    for i, n in enumerate(results, 1):
        print(f"\n{i}. {n['title'][:50]}")
        print(f"   æ—¥æœŸ: {n['publish_date']} | åˆ†ç±»: {n['category']}")
        if n.get('key_points'):
            print(f"   è¦ç‚¹: {'; '.join(n['key_points'])}")


if __name__ == "__main__":
    main()
