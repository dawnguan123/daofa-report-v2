#!/usr/bin/env python3
"""
ä¸­å›½æ–°é—»ç½‘é€šç”¨æ–°é—»è·å–å™¨
æ”¯æŒä¸åŒé¢‘é“ï¼š/china/, /society/, /gn/ ç­‰
è‡ªåŠ¨ä¿å­˜åˆ°Tursoæ•°æ®åº“
åŒ…å«AIä¸“ä¸šæ€»ç»“åŠŸèƒ½
"""
import requests
import re
import json
import time
import sqlite3
from bs4 import BeautifulSoup
from datetime import datetime

# é…ç½®
DB_PATH = "/Users/guanliming/dailynews/turso/textbook_full.db"
OUTPUT_DIR = "/Users/guanliming/dailynews/output"


class ChinaNewsFetcher:
    """ä¸­å›½æ–°é—»ç½‘æ–°é—»è·å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
    
    def get_dh_area_class(self, soup):
        """è‡ªåŠ¨å‘ç° dh å¼€å¤´çš„åŒºåŸŸç±»å"""
        for div in soup.find_all('div', class_=True):
            classes = div.get('class', [])
            for cls in classes:
                if cls.startswith('dh'):
                    return cls
        return None
    
    def extract_key_points(self, content, title):
        """ä»å†…å®¹ä¸­æå–å…³é”®è¦ç‚¹"""
        key_points = []
        
        # æ¸…ç†å†…å®¹
        content = re.sub(r'\s+', ' ', content)
        content = re.sub(r'\d{1,2}:\d{2}', '', content)
        
        # æå–æ¶‰åŠæœºæ„
        org_patterns = [
            r'([^\s]{2,8}(?:éƒ¨|å§”|å±€|åŠ|æ”¿åºœ|ç›‘å§”|èˆªå¤©å±€|åŠå…¬å®¤))',
            r'([^\s]{2,10}(?:å…¬å¸|ä¼ä¸š|ç ”ç©¶æ‰€|å·¥ç¨‹åŠ))',
        ]
        for pattern in org_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if 2 <= len(match) <= 6 and 'æ®' not in match and 'è®°è€…' not in match:
                    if match not in str(key_points):
                        key_points.append(f"æœºæ„ï¼š{match}")
        
        # æå–å…³é”®äº‹ä»¶
        event_patterns = [
            r'([^\s]{4,12}(?:è¯•éªŒ|å‘å°„|æˆåŠŸ|çªç ´|å‘å¸ƒ|å®æ–½))',
            r'([^\s]{3,10}(?:å·¥ç¨‹|è®¡åˆ’|é¡¹ç›®|ç«ç®­|é£èˆ¹))',
        ]
        for pattern in event_patterns:
            matches = re.findall(pattern, content)
            for match in matches[:3]:
                if 3 <= len(match) <= 10 and match not in str(key_points):
                    key_points.append(f"äº‹ä»¶ï¼š{match}")
        
        # æå–å…³é”®æ•°å­—
        number_patterns = [
            r'(\d{4}å¹´)',
            r'(\d+æœˆ\d+æ—¥)',
            r'(\d+\.\d+%)',
            r'(\d+ä¸‡|\d+äº¿)',
        ]
        for pattern in number_patterns:
            matches = re.findall(pattern, content)
            for match in matches[:2]:
                if match not in str(key_points):
                    key_points.append(f"æ•°æ®ï¼š{match}")
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_points = []
        seen = set()
        for point in key_points:
            key = point[:12]
            if key not in seen:
                seen.add(key)
                unique_points.append(point)
        
        return unique_points[:4]
    
    def fetch_list(self, url, max_news=5):
        """è·å–æ–°é—»åˆ—è¡¨"""
        print(f"\nğŸ“° è®¿é—®: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"  âš ï¸ çŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            news_list = []
            seen_urls = set()
            seen_titles = set()
            
            # æ–¹æ³•1: dh å¼€å¤´çš„æ»šåŠ¨åŒºåŸŸ
            dh_class = self.get_dh_area_class(soup)
            if dh_class:
                print(f"  âœ… æ‰¾åˆ° dh åŒºåŸŸ: {dh_class}")
                area = soup.select_one(f'.{dh_class}')
                if area:
                    links = area.find_all('a')
                    for a in links:
                        news = self._parse_link(a)
                        if news and self._is_valid(news):
                            self._add_news(news, news_list, seen_urls, seen_titles)
            
            # æ–¹æ³•2: news-list åŒºåŸŸ
            if len(news_list) < max_news:
                print(f"  ğŸ” æœç´¢ news-list...")
                for div in soup.select('.news-list')[:3]:
                    for a in div.find_all('a')[:max_news * 2]:
                        news = self._parse_link(a)
                        if news and self._is_valid(news):
                            self._add_news(news, news_list, seen_urls, seen_titles)
            
            print(f"  ğŸ“Š è·å– {len(news_list)} æ¡æ–°é—»")
            return news_list[:max_news]
            
        except Exception as e:
            print(f"  âš ï¸ é”™è¯¯: {e}")
            return []
    
    def _parse_link(self, a):
        """è§£æé“¾æ¥"""
        title = a.get_text(strip=True)
        href = a.get('href', '')
        
        if not title or not href:
            return None
        
        # è¡¥å…¨URL
        if href.startswith('//'):
            href = 'https:' + href
        elif href.startswith('/'):
            href = 'https://www.chinanews.com.cn' + href
        
        # è¿‡æ»¤æœ‰æ•ˆé“¾æ¥
        if not href or 'chinanews' not in href:
            return None
        
        # è·³è¿‡è§†é¢‘ã€ç›¸å†Œç­‰é“¾æ¥
        skip_patterns = ['video', 'tv', 'photo', 'shipin', 'shipin', 'pic']
        if any(p in href.lower() for p in skip_patterns):
            return None
        
        # è·³è¿‡å¤ªçŸ­çš„æ ‡é¢˜
        if len(title) < 6:
            return None
        
        return {'title': title, 'url': href}
    
    def _is_valid(self, news):
        """éªŒè¯æ–°é—»æœ‰æ•ˆæ€§"""
        title = news.get('title', '')
        url = news.get('url', '')
        
        if not title or not url:
            return False
        
        skip_titles = ['è§†é¢‘', 'ç›´æ’­', 'æ»šåŠ¨', 'ä¸“é¢˜', 'å¹¿å‘Š', 'æ’è¡Œæ¦œ']
        if any(t in title for t in skip_titles):
            return False
        
        return True
    
    def _add_news(self, news, news_list, seen_urls, seen_titles):
        """æ·»åŠ æ–°é—»åˆ°åˆ—è¡¨"""
        url = news.get('url', '')
        title = news.get('title', '')
        
        if url in seen_urls or title in seen_titles:
            return
        
        seen_urls.add(url)
        seen_titles.add(title)
        news_list.append(news)
    
    def fetch_detail(self, url):
        """è·å–æ–°é—»è¯¦æƒ…"""
        print(f"  ğŸ“„ è·å–è¯¦æƒ…: {url[:60]}...")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è·å–å‘å¸ƒæ—¶é—´
            pub_date = None
            time_elem = soup.select_one('.pub-time, .publish-time, .time, time')
            if time_elem:
                time_text = time_elem.get_text(strip=True)
                pub_date = self._parse_date(time_text)
            
            # è·å–å†…å®¹
            content = ""
            content_elem = soup.select_one('.content, .article-content, .text, .news-content, #article')
            if content_elem:
                # è·å–çº¯æ–‡æœ¬
                content = content_elem.get_text(strip=True)
                
                # æ¸…ç†å†…å®¹å™ªå£°
                content = re.sub(r'æ¥æº[ï¼š:]\s*[^\s]+', '', content)
                content = re.sub(r'ä½œè€…[ï¼š:]\s*[^\s]+', '', content)
                content = re.sub(r'è´£ä»»ç¼–è¾‘[ï¼š:]\s*[^\s]+', '', content)
                content = re.sub(r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥\d{1,2}:\d{2}', '', content)
                content = re.sub(r'åˆ†äº«åˆ°[^\n]*', '', content)
                content = re.sub(r'å¤§å­—ä½“å°å­—ä½“[^\n]*', '', content)
                content = re.sub(r'\s+', ' ', content)
                content = content.strip()
            
            # è·å–æ¥æº
            source = "ä¸­å›½æ–°é—»ç½‘"
            source_elem = soup.select_one('.source, .from, .pub-source')
            if source_elem:
                source_text = source_elem.get_text(strip=True)
                match = re.search(r'æ¥æº[ï¼š:]\s*([^\s]+)', source_text)
                if match:
                    source = match.group(1)
            
            return {
                'title': '',
                'url': url,
                'content': content,
                'publish_date': pub_date or datetime.now().strftime('%Y-%m-%d'),
                'source': source,
            }
            
        except Exception as e:
            print(f"  âš ï¸ è¯¦æƒ…è·å–å¤±è´¥: {e}")
            return None
    
    def _parse_date(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        try:
            # åŒ¹é…æ ¼å¼: 2026-02-11 10:30 æˆ– 2026å¹´02æœˆ11æ—¥
            match = re.search(r'(\d{4})[-å¹´](\d{1,2})[-æœˆ](\d{1,2})', date_str)
            if match:
                return f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
        except:
            pass
        return None
    
    def process_news(self, news_item, category="æ—¶æ”¿"):
        """å¤„ç†å•æ¡æ–°é—»"""
        url = news_item.get('url', '')
        title = news_item.get('title', '')
        
        # è·å–è¯¦æƒ…
        detail = self.fetch_detail(url)
        if not detail:
            return None
        
        # åˆå¹¶æ•°æ®
        news = {
            'title': title or detail.get('title', ''),
            'url': url,
            'source': detail.get('source', 'ä¸­å›½æ–°é—»ç½‘'),
            'publish_date': detail.get('publish_date', datetime.now().strftime('%Y-%m-%d')),
            'content': detail.get('content', ''),
            'category': category,
            'channel': url.split('/')[3] if len(url.split('/')) > 3 else 'news',
        }
        
        # ç”Ÿæˆæ‘˜è¦
        news['summary'] = self._generate_summary(news['content'])
        
        # æå–å…³é”®è¦ç‚¹
        news['key_points'] = self.extract_key_points(news['content'], news['title'])
        
        # ç”ŸæˆAIæ€»ç»“
        news['ai_summary'] = self.generate_ai_summary(news)
        
        # å…ƒæ•°æ®
        news['metadata'] = json.dumps({
            'rank': 0,
            'processed_at': datetime.now().isoformat()
        }, ensure_ascii=False)
        
        return news
    
    def _generate_summary(self, content):
        """ç”Ÿæˆæ‘˜è¦"""
        if not content:
            return ""
        
        # æ¸…ç†å†…å®¹å™ªå£°
        content = re.sub(r'æ¥æº[ï¼š:]\s*[^\s]+', '', content)
        content = re.sub(r'ä½œè€…[ï¼š:]\s*[^\s]+', '', content)
        content = re.sub(r'è´£ä»»ç¼–è¾‘[ï¼š:]\s*[^\s]+', '', content)
        # æ¸…ç†æ—¥æœŸæ—¶é—´æ ¼å¼
        content = re.sub(r'\d{4}[-_]\d{1,2}[-_]\d{1,2}\s*\d{0,2}:\d{0,2}(?::\d{0,2})?', '', content)
        content = re.sub(r'\d{1,2}:\d{2}', '', content)
        # æ¸…ç†æ‹¬å·å†…çš„è®°è€…å
        content = re.sub(r'\([^\)]*è®°è€…[^\)]*\)', '', content)
        # æ¸…ç†åˆ†äº«é“¾æ¥ç­‰
        content = re.sub(r'åˆ†äº«åˆ°[^\n]*', '', content)
        content = re.sub(r'å¤§å­—ä½“å°å­—ä½“[^\n]*', '', content)
        content = re.sub(r'\s+', ' ', content)
        content = content.strip()
        
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ®µè½ï¼ˆå¥å·åï¼‰ï¼Œè·³è¿‡é‡å¤çš„æ ‡é¢˜
        first_period = content.find('.')
        if first_period > 0 and first_period < 100:
            content = content[first_period+1:]
        
        # å–å‰150å­—
        if len(content) > 150:
            return content[:150].strip() + "..."
        return content.strip()
    
    def generate_ai_summary(self, news):
        """ç”ŸæˆAIä¸“ä¸šæ€»ç»“"""
        title = news.get('title', '')
        content = news.get('content', '')[:2000]
        category = news.get('category', 'æ—¶æ”¿')
        key_points = news.get('key_points', [])
        
        summary_parts = []
        
        # 1. æ–°é—»ç»¼è¿°
        summary_parts.append("ã€æ–°é—»ç»¼è¿°ã€‘")
        summary_parts.append(content[:300].replace('\n', ' ') + "...")
        
        # 2. æ ¸å¿ƒè¦ç‚¹
        summary_parts.append("\nã€æ ¸å¿ƒè¦ç‚¹ã€‘")
        if key_points:
            for point in key_points[:4]:
                summary_parts.append(f"â€¢ {point}")
        else:
            summary_parts.append("â€¢ é‡è¦äº‹ä»¶å‘å±•")
            summary_parts.append("â€¢ ç›¸å…³æ”¿ç­–æªæ–½")
        
        # 3. é“æ³•å…³è”
        summary_parts.append("\nã€é“æ³•å…³è”ã€‘")
        summary_parts.append(self._get_daofa_correlation(title, content, category))
        
        # 4. æ€è€ƒé—®é¢˜
        summary_parts.append("\nã€æ€è€ƒé—®é¢˜ã€‘")
        questions = self._get_thinking_questions(title, category)
        for q in questions:
            summary_parts.append(f"â€¢ {q}")
        
        return '\n'.join(summary_parts)
    
    def _get_daofa_correlation(self, title, content, category):
        """è·å–é“æ³•è¯¾ç¨‹å…³è”"""
        text = title + content
        
        correlations = [
            (['èˆªå¤©', 'æœˆçƒ', 'æ¢æµ‹', 'é£èˆ¹', 'ç«ç®­', 'ç©ºé—´ç«™'], 
             'ä¹å¹´çº§ä¸Šå†Œã€Šåˆ›æ–°é©±åŠ¨å‘å±•ã€‹', 
             'ä½“ç°æˆ‘å›½ç§‘æŠ€è‡ªç«‹è‡ªå¼ºã€èˆªå¤©å¼ºå›½å»ºè®¾'),
            
            (['å°ç‹¬', 'ä¸¤å²¸', 'å°æ¹¾', 'ç¥–å›½ç»Ÿä¸€', 'å›½å°åŠ'],
             'ä¹å¹´çº§ä¸Šå†Œã€Šä¸­åä¸€å®¶äº²ã€‹',
             'ä½“ç°ç»´æŠ¤å›½å®¶ç»Ÿä¸€ã€æ°‘æ—å›¢ç»“'),
             
            (['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•', 'æ‹›ç”Ÿ'],
             'ä¹å¹´çº§ä¸Šå†Œã€Šè¸ä¸Šå¼ºå›½ä¹‹è·¯ã€‹',
             'ä½“ç°æ•™è‚²å¼ºå›½ã€ç§‘æŠ€å¼ºå›½æˆ˜ç•¥'),
             
            (['è¿æ³•', 'è¿çºª', 'è…è´¥', 'è´ªæ±¡', 'å—è´¿', 'ç›‘å¯Ÿ'],
             'ä¸ƒå¹´çº§ä¸‹å†Œã€Šèµ°è¿›æ³•æ²»å¤©åœ°ã€‹',
             'ä½“ç°ä¾æ³•æ²»å›½ã€åè…å€¡å»‰'),
             
            (['ç»æµ', 'å‘å±•', 'æ”¹é©', 'ä¼ä¸š', 'å¸‚åœº', 'äº§ä¸š'],
             'ä¹å¹´çº§ä¸Šå†Œã€Šåˆ›æ–°é©±åŠ¨å‘å±•ã€‹',
             'ä½“ç°é«˜è´¨é‡å‘å±•ã€æ–°å‘å±•ç†å¿µ'),
             
            (['æ°‘ç”Ÿ', 'å°±ä¸š', 'åŒ»ç–—', 'å…»è€', 'ç¤¾ä¿', 'ä½æˆ¿'],
             'å…«å¹´çº§ä¸Šå†Œã€Šç¤¾ä¼šç”Ÿæ´»ã€‹',
             'ä½“ç°ä»¥äººæ°‘ä¸ºä¸­å¿ƒçš„å‘å±•æ€æƒ³'),
             
            (['ç¯ä¿', 'ç”Ÿæ€', 'ç»¿è‰²', 'ç¢³ä¸­å’Œ', 'æ±¡æŸ“é˜²æ²»'],
             'ä¸ƒå¹´çº§ä¸Šå†Œã€Šç”Ÿå‘½å¥åº·ã€‹',
             'ä½“ç°ç”Ÿæ€æ–‡æ˜å»ºè®¾ã€ç»¿æ°´é’å±±å°±æ˜¯é‡‘å±±é“¶å±±'),
        ]
        
        for keywords, chapter, desc in correlations:
            if any(k in text for k in keywords):
                return f"{chapter}\nç« èŠ‚å…³è”ï¼š{desc}\nåŒ¹é…ï¼š{', '.join(keywords[:3])}"
        
        return f"{category}ç›¸å…³ç« èŠ‚\nä½“ç°å›½å®¶å‘å±•ä¸ç¤¾ä¼šè¿›æ­¥çš„é‡è¦è®®é¢˜\nåŒ¹é…ï¼š{category}"
    
    def _get_thinking_questions(self, title, category):
        """è·å–æ€è€ƒé—®é¢˜"""
        questions = {
            'æ—¶æ”¿': [
                'è¿™ä¸€æ”¿ç­–å¯¹æ™®é€šäººç”Ÿæ´»æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ',
                'æ–°é—»åæ˜ äº†å“ªäº›ç¤¾ä¼šå‘å±•è¶‹åŠ¿ï¼Ÿ',
            ],
            'ç§‘æŠ€': [
                'ç§‘æŠ€è¿›æ­¥å¯¹å›½å®¶å‘å±•æœ‰ä»€ä¹ˆé‡è¦æ„ä¹‰ï¼Ÿ',
                'ä½œä¸ºé’å°‘å¹´å¦‚ä½•åŸ¹å…»ç§‘æŠ€åˆ›æ–°ç²¾ç¥ï¼Ÿ',
            ],
            'ç¤¾ä¼š': [
                'è¿™ä¸€ç¤¾ä¼šç°è±¡è¯´æ˜äº†ä»€ä¹ˆï¼Ÿ',
                'æˆ‘ä»¬èƒ½ä¸ºè§£å†³ç¤¾ä¼šé—®é¢˜åšä»€ä¹ˆï¼Ÿ',
            ],
            'ä¸¤å²¸': [
                'ä¸ºä»€ä¹ˆè¯´ä¸¤å²¸åŒèƒæ˜¯ä¸€å®¶äººï¼Ÿ',
                'é’å°‘å¹´å¦‚ä½•ä¸ºç¥–å›½ç»Ÿä¸€è´¡çŒ®åŠ›é‡ï¼Ÿ',
            ],
            'æ³•å¾‹': [
                'è¿™ä¸€æ¡ˆä¾‹è¯´æ˜äº†ä»€ä¹ˆæ³•å¾‹åŸåˆ™ï¼Ÿ',
                'é’å°‘å¹´åº”è¯¥å¦‚ä½•å¢å¼ºæ³•æ²»æ„è¯†ï¼Ÿ',
            ],
        }
        return questions.get(category, ['è¿™ä¸€äº‹ä»¶æœ‰ä»€ä¹ˆé‡è¦æ„ä¹‰ï¼Ÿ', 'å¯¹ä½ æœ‰ä»€ä¹ˆå¯å‘ï¼Ÿ'])
    
    def save_to_db(self, news_list, channel="china"):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        if not news_list:
            return 0
        
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        saved_count = 0
        updated_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        for news in news_list:
            if not news.get('title'):
                continue
            
            try:
                cursor.execute("""
                    INSERT OR REPLACE INTO news_articles 
                    (title, url, source, publish_date, content, summary, category, 
                     channel, key_points, ai_summary, metadata, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    news.get('title', ''),
                    news.get('url', ''),
                    news.get('source', 'ä¸­å›½æ–°é—»ç½‘'),
                    news.get('publish_date', datetime.now().strftime('%Y-%m-%d')),
                    news.get('content', ''),
                    news.get('summary', ''),
                    news.get('category', 'æ—¶æ”¿'),
                    news.get('channel', channel),
                    json.dumps(news.get('key_points', []), ensure_ascii=False),
                    news.get('ai_summary', ''),
                    news.get('metadata', '{}'),
                    updated_at,
                ))
                saved_count += 1
                
            except Exception as e:
                print(f"  âš ï¸ ä¿å­˜å¤±è´¥: {news.get('title', '')[:30]}... - {e}")
        
        conn.commit()
        conn.close()
        
        print(f"  ğŸ’¾ ä¿å­˜ {saved_count} æ¡æ–°é—»åˆ°æ•°æ®åº“")
        return saved_count
    
    def run(self, channel_url, max_news=5):
        """ä¸»è¿è¡Œå‡½æ•°"""
        print("=" * 60)
        print("ğŸš¢ å¯åŠ¨ä¸­å›½æ–°é—»ç½‘æ–°é—»è·å–å™¨")
        print("=" * 60)
        
        # è§£æé¢‘é“
        if 'china' in channel_url:
            channel = 'china'
            category = 'æ—¶æ”¿'
        elif 'society' in channel_url:
            channel = 'society'
            category = 'ç¤¾ä¼š'
        else:
            channel = 'news'
            category = 'æ—¶æ”¿'
        
        # è·å–åˆ—è¡¨
        news_list = self.fetch_list(channel_url, max_news)
        
        if not news_list:
            print("âš ï¸ æœªè·å–åˆ°æ–°é—»")
            return []
        
        # å¤„ç†è¯¦æƒ…
        print("\nğŸ”„ å¤„ç†æ–°é—»è¯¦æƒ…...")
        processed_news = []
        for i, news in enumerate(news_list):
            print(f"  [{i+1}/{len(news_list)}] {news.get('title', '')[:40]}...")
            detail = self.process_news(news, category)
            if detail:
                processed_news.append(detail)
        
        # ä¿å­˜
        saved = self.save_to_db(processed_news, channel)
        
        print("\n" + "=" * 60)
        print(f"âœ… å®Œæˆï¼å…±å¤„ç† {len(processed_news)} æ¡ï¼Œä¿å­˜ {saved} æ¡")
        print("=" * 60)
        
        return processed_news


if __name__ == '__main__':
    import sys
    
    url = sys.argv[1] if len(sys.argv) > 1 else 'https://www.chinanews.com.cn/china/'
    max_news = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    fetcher = ChinaNewsFetcher()
    fetcher.run(url, max_news)
