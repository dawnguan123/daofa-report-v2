#!/usr/bin/env python3
"""
PDFè¯¾æœ¬è§£æå…¥åº“è„šæœ¬
æå–æ‰€æœ‰7ä¸ªPDFçš„ç« èŠ‚å†…å®¹å’ŒçŸ¥è¯†ç‚¹ï¼Œå­˜å…¥SQLite
"""
import os
import re
import sqlite3
import json
import pdfplumber
from datetime import datetime

# PDFæ–‡ä»¶åˆ—è¡¨
PDF_FILES = [
    ("ä¸ƒå¹´çº§ä¸Šå†Œ", "é“æ³•è¯¾æœ¬/ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ é“å¾·ä¸æ³•æ²» ä¸ƒå¹´çº§ä¸Šå†Œ.pdf"),
    ("ä¸ƒå¹´çº§ä¸‹å†Œ", "é“æ³•è¯¾æœ¬/ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ é“å¾·ä¸æ³•æ²» ä¸ƒå¹´çº§ä¸‹å†Œ.pdf"),
    ("å…«å¹´çº§ä¸Šå†Œ", "é“æ³•è¯¾æœ¬/ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ é“å¾·ä¸æ³•æ²» å…«å¹´çº§ä¸Šå†Œ.pdf"),
    ("å…«å¹´çº§ä¸‹å†Œ", "é“æ³•è¯¾æœ¬/ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ é“å¾·ä¸æ³•æ²» å…«å¹´çº§ä¸‹å†Œ.pdf"),
    ("ä¹å¹´çº§ä¸Šå†Œ", "é“æ³•è¯¾æœ¬/ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ é“å¾·ä¸æ³•æ²» ä¹å¹´çº§ä¸Šå†Œ.pdf"),
    ("ä¹å¹´çº§ä¸‹å†Œ", "é“æ³•è¯¾æœ¬/ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦ é“å¾·ä¸æ³•æ²» ä¹å¹´çº§ä¸‹å†Œ.pdf"),
    ("å­¦ç”Ÿè¯»æœ¬", "é“æ³•è¯¾æœ¬/å­¦ç”Ÿè¯»æœ¬Â·åˆä¸­.pdf"),
]

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ""
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_chapters_from_pdf(pdf_path, book_name):
    """ä»PDFæå–ç« èŠ‚"""
    chapters = []
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"\nğŸ“– å¤„ç† {book_name} ({len(pdf.pages)} é¡µ)")
            
            current_chapter = None
            current_section = None
            content_buffer = []
            page_num = 1
            
            for page in pdf.pages:
                text = page.extract_text()
                if not text:
                    page_num += 1
                    continue
                
                text = clean_text(text)
                
                # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜ï¼ˆå¸¸è§æ ¼å¼ï¼‰
                chapter_patterns = [
                    r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+å•å…ƒ\s+([^\n]+)',
                    r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+è¯¾\s+([^\n]+)',
                    r'^ç¬¬ä¸€å•å…ƒ\s*$',
                    r'^ç¬¬äºŒå•å…ƒ\s*$',
                    r'^ç¬¬ä¸‰å•å…ƒ\s*$',
                    r'^ç¬¬å››å•å…ƒ\s*$',
                ]
                
                lines = text.split('ã€‚')
                for line in lines[:10]:  # åªæ£€æŸ¥å‰å‡ è¡Œ
                    for pattern in chapter_patterns:
                        match = re.match(pattern, line)
                        if match:
                            # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚
                            if current_chapter:
                                chapters.append({
                                    "book": book_name,
                                    "chapter": current_chapter,
                                    "section": current_section or "",
                                    "content": clean_text(' '.join(content_buffer)),
                                    "page": page_num
                                })
                            
                            current_chapter = match.group(1) if match.group(1) else line.strip()
                            current_section = ""
                            content_buffer = [line]
                            break
                    else:
                        # ä¸æ˜¯ç« èŠ‚æ ‡é¢˜ï¼Œæ·»åŠ åˆ°å†…å®¹
                        if current_chapter and len(line) > 10:
                            content_buffer.append(line)
                
                page_num += 1
            
            # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
            if current_chapter and content_buffer:
                chapters.append({
                    "book": book_name,
                    "chapter": current_chapter,
                    "section": current_section,
                    "content": clean_text(' '.join(content_buffer)),
                    "page": page_num
                })
    
    except Exception as e:
        print(f"  âœ— é”™è¯¯: {e}")
    
    return chapters

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    conn = sqlite3.connect('turso/textbook_full.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS textbook_chapters (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            book_name TEXT NOT NULL,
            chapter_title TEXT NOT NULL,
            section_title TEXT,
            page_range TEXT,
            content TEXT,
            content_summary TEXT,
            keywords TEXT,
            embedding BLOB,
            metadata TEXT,
            created_at DATETIME,
            updated_at DATETIME
        )
    ''')
    
    cursor.execute('DELETE FROM textbook_chapters')
    conn.commit()
    return conn

def generate_summary(content, max_length=150):
    """ç”Ÿæˆå†…å®¹æ‘˜è¦"""
    if not content:
        return ""
    
    # å–å‰200å­—ä½œä¸ºæ‘˜è¦
    summary = content[:200]
    if len(content) > 200:
        summary += "..."
    return summary

def extract_keywords(content):
    """æå–å…³é”®è¯"""
    if not content:
        return []
    
    # å¸¸è§ä¸»é¢˜è¯
    theme_words = [
        'æ•™è‚²', 'æ³•å¾‹', 'æƒåˆ©', 'ä¹‰åŠ¡', 'è´£ä»»', 'é“å¾·', 'è¯šä¿¡', 'å‹è°Š', 'äº²æƒ…',
        'ç”Ÿå‘½', 'æˆé•¿', 'é’æ˜¥', 'æ¢¦æƒ³', 'è‡ªä¿¡', 'è‡ªå¼º', 'æ„Ÿæ©', 'å¥‰çŒ®', 'åˆ›æ–°',
        'æ³•æ²»', 'æ°‘ä¸»', 'å¹³ç­‰', 'å…¬æ­£', 'å’Œè°', 'æ–‡æ˜', 'å‹å–„', 'æ•¬ä¸š', 'çˆ±å›½',
        'å¸‚åœºç»æµ', 'æ¶ˆè´¹', 'åŠ³åŠ¨', 'æ–‡åŒ–', 'ä¼ ç»Ÿ', 'ç¾å¾·', 'è´£ä»»', 'æ‹…å½“',
        'æœªæˆå¹´', 'ä¿æŠ¤', 'å®‰å…¨', 'å¥åº·', 'å¿ƒç†', 'ç½‘ç»œ', 'åª’ä½“', 'èˆ†è®º'
    ]
    
    found = []
    for word in theme_words:
        if word in content:
            found.append(word)
    
    return found[:10]

def save_to_db(conn, chapters):
    """ä¿å­˜åˆ°æ•°æ®åº“"""
    cursor = conn.cursor()
    
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    total = 0
    
    for ch in chapters:
        if len(ch['content']) < 50:  # è·³è¿‡å¤ªçŸ­çš„
            continue
        
        content_summary = generate_summary(ch['content'])
        keywords = json.dumps(extract_keywords(ch['content']), ensure_ascii=False)
        
        cursor.execute('''
            INSERT INTO textbook_chapters 
            (book_name, chapter_title, section_title, page_range, content, 
             content_summary, keywords, embedding, metadata, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', [
            ch['book'],
            ch['chapter'],
            ch['section'],
            f"{ch['page']}-{ch['page']+5}",
            ch['content'],
            content_summary,
            keywords,
            json.dumps([0.0] * 768),  # ç©ºçš„embedding
            json.dumps(ch),
            now,
            now
        ])
        total += 1
    
    conn.commit()
    return total

def main():
    print("=" * 60)
    print("ğŸ“š PDFè¯¾æœ¬è§£æå…¥åº“")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    conn = init_database()
    
    all_chapters = []
    
    for book_name, pdf_path in PDF_FILES:
        if not os.path.exists(pdf_path):
            print(f"\nâš ï¸  è·³è¿‡: {pdf_path} ä¸å­˜åœ¨")
            continue
        
        print(f"\nğŸ“„ å¤„ç†: {book_name}")
        chapters = extract_chapters_from_pdf(pdf_path, book_name)
        print(f"  â†’ æå– {len(chapters)} ä¸ªç« èŠ‚")
        
        for ch in chapters[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"     - {ch['chapter'][:30]}...")
        
        all_chapters.extend(chapters)
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    print(f"\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
    saved = save_to_db(conn, all_chapters)
    print(f"  â†’ æˆåŠŸä¿å­˜ {saved} ä¸ªç« èŠ‚")
    
    conn.close()
    print("\nâœ… å®Œæˆ!")
    print(f"ğŸ“ æ•°æ®åº“: turso/textbook_full.db")

if __name__ == "__main__":
    main()
