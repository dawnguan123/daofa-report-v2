#!/usr/bin/env python3
"""
æ—¶äº‹æ–°é—»è·å–å™¨ - é“æ³•æ—¶äº‹æŠ¥å‘Šä¸“ç”¨
æ”¯æŒä»ä¸­å›½æ–°é—»ç½‘ç­‰ä¸»æµåª’ä½“è·å–æ—¶æ”¿æ–°é—»
"""
import requests
import re
import json
from datetime import datetime
from bs4 import BeautifulSoup
import time

class NewsFetcher:
    """æ—¶æ”¿æ–°é—»è·å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
        # é…ç½®è¦æŠ“å–çš„æ–°é—»æº
        self.sources = [
            {
                'id': 'chinanews_gn',
                'name': 'ä¸­å›½æ–°é—»ç½‘-å›½å†…',
                'url': 'https://www.chinanews.com.cn/gn/',
                'type': 'list',
                'selector': 'a[href*="/gn/2026/"]',
                'enabled': True
            },
        ]
    
    def fetch_list(self, source):
        """ä»åˆ—è¡¨é¡µè·å–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥"""
        try:
            response = self.session.get(source['url'], timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    çŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.select(source['selector'])
            
            news_list = []
            seen_urls = set()
            
            for link in links[:25]:
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                if not href or not title or len(title) < 10:
                    continue
                
                # æ¸…ç†URL
                if href.startswith('//'):
                    full_url = 'https:' + href
                elif href.startswith('/'):
                    full_url = 'https://www.chinanews.com.cn' + href
                else:
                    full_url = href
                
                if full_url in seen_urls:
                    continue
                seen_urls.add(full_url)
                
                news_list.append({
                    'title': title,
                    'url': full_url,
                    'source': source['name'],
                    'source_id': source['id'],
                    'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                    'category': self._guess_category(title),
                    'content': '',
                    'summary': '',
                    'key_points': []
                })
            
            return news_list
            
        except Exception as e:
            print(f"    è·å–å¤±è´¥: {e}")
            return []
    
    def fetch_detail(self, news):
        """ä¸‹é’»åˆ°è¯¦æƒ…é¡µè·å–å†…å®¹"""
        try:
            response = self.session.get(news['url'], timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                return news
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è·å–æ‰€æœ‰æ®µè½
            all_p = soup.find_all('p')
            if all_p:
                texts = [p.get_text(strip=True) for p in all_p]
                texts = [t for t in texts if len(t) > 20]
                content = ' '.join(texts)
                
                # ç”Ÿæˆæ‘˜è¦
                news['content'] = content
                news['summary'] = self._generate_summary(content)
                news['key_points'] = self._extract_key_points(content, news['title'])
            
            return news
            
        except Exception as e:
            print(f"    è¯¦æƒ…è·å–å¤±è´¥: {e}")
            return news
    
    def _guess_category(self, title):
        """æ ¹æ®æ ‡é¢˜çŒœæµ‹åˆ†ç±»"""
        title_lower = title.lower()
        
        categories = {
            'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•', 'å‡å­¦'],
            'æ³•å¾‹': ['æ³•å¾‹', 'æ³•é™¢', 'æ£€å¯Ÿé™¢', 'å…¬å®‰', 'å¸æ³•', 'çŠ¯ç½ª', 'æœªæˆå¹´äºº', 'æ£€å¯Ÿ'],
            'ç»æµ': ['ç»æµ', 'GDP', 'CPI', 'ç»Ÿè®¡å±€', 'å¸‚åœº', 'ä¼ä¸š', 'æ¶ˆè´¹', 'äº§ä¸š'],
            'æ”¿æ²»': ['æ”¿åºœ', 'ä¼šè®®', 'æ”¿ç­–', 'é¢†å¯¼äºº', 'ä¹ è¿‘å¹³', 'æå¼º', 'è®²è¯', 'å…šå»º', 'å†›é˜Ÿ', 'å»ºå†›'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'åŒ»ç–—', 'ç¤¾ä¿', 'å°±ä¸š', 'ä½æˆ¿', 'ç¯ä¿'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'èˆªå¤©', 'æœˆçƒ', 'äººå·¥æ™ºèƒ½', 'AI', 'åˆ›æ–°'],
            'ä¸¤å²¸': ['å°æ¹¾', 'ä¸¤å²¸', 'å›½å°åŠ', 'å°æµ·', 'ç»Ÿä¸€'],
            'å¤–äº¤': ['å¤–äº¤', 'å¤–é•¿', 'è”åˆå›½', 'å›½é™…', 'å³°ä¼š']
        }
        
        for cat, keywords in categories.items():
            for kw in keywords:
                if kw in title:
                    return cat
        
        return 'ç»¼åˆ'
    
    def _generate_summary(self, content):
        """ç”Ÿæˆæ‘˜è¦"""
        if not content or len(content) < 50:
            return ""
        
        summary = content[:300]
        if len(content) > 300:
            summary += "..."
        
        return summary
    
    def _extract_key_points(self, content, title):
        """æå–å…³é”®è¦ç‚¹"""
        key_points = []
        
        # æå–äººå
        names = re.findall(r'ä¹ è¿‘å¹³|æå¼º|ä¸è–›ç¥¥|æå¸Œ|ç‹æ¯…|èµµä¹é™…', content)
        if names:
            key_points.append(f"æ¶‰åŠäººç‰©ï¼š{', '.join(set(names[:2]))}")
        
        # æå–æœºæ„
        orgs = re.findall(r'ä¸­å…±ä¸­å¤®|å›½åŠ¡é™¢|ä¸­å¤®å†›å§”|å›½åŠ¡é™¢å°åŠ|å·¥ä¿¡éƒ¨|ç§‘æŠ€éƒ¨|ä½å»ºéƒ¨|å¸‚åœºç›‘ç®¡æ€»å±€|å›½å®¶ç»Ÿè®¡å±€', content)
        if orgs:
            key_points.append(f"æ¶‰åŠæœºæ„ï¼š{', '.join(set(orgs[:2]))}")
        
        # æå–å…³é”®äº‹ä»¶
        events = re.findall(r'(æœˆçƒæ¢æµ‹|è½½äººèˆªå¤©|åè…è´¥|ä¸¤å²¸|ç§‘æŠ€æœåŠ¡|æ ‡å‡†ä½“ç³»|å»ºå†›|æ”¹é©|å‘å±•)', content)
        if events:
            key_points.append(f"å…³é”®äº‹ä»¶ï¼š{', '.join(set(events[:2]))}")
        
        # æå–æ•°æ®
        data = re.findall(r'(\d+å¹´|\d+æœˆ\d+æ—¥|\d+\.?\d*%)', content)
        if data:
            key_points.append(f"å…³é”®æ•°æ®ï¼š{data[0]}")
        
        return key_points
    
    def get_news(self, max_news=5):
        """è·å–æ–°é—»ä¸»å…¥å£"""
        all_news = []
        
        for source in self.sources:
            if not source['enabled']:
                continue
            
            print(f"  ğŸŒ æ­£åœ¨ä» {source['name']} è·å–...")
            
            # 1. è·å–æ ‡é¢˜åˆ—è¡¨
            news_list = self.fetch_list(source)
            print(f"    è·å– {len(news_list)} æ¡æ ‡é¢˜")
            
            if not news_list:
                continue
            
            # 2. ä¸‹é’»è·å–è¯¦æƒ…
            print(f"  ğŸ“° ä¸‹é’»è·å–è¯¦æƒ…...")
            enriched_news = []
            for i, news in enumerate(news_list[:max_news], 1):
                print(f"    [{i}] {news['title'][:25]}...")
                enriched = self.fetch_detail(news)
                enriched_news.append(enriched)
                time.sleep(0.3)
            
            all_news.extend(enriched_news)
            break
        
        return all_news[:max_news]


def fetch_news():
    """ä¾¿æ·å‡½æ•°"""
    fetcher = NewsFetcher()
    return fetcher.get_news(max_news=5)


if __name__ == "__main__":
    news = fetch_news()
    
    print(f"\nè·å–åˆ° {len(news)} æ¡æ–°é—»:\n")
    for i, n in enumerate(news, 1):
        print(f"{i}. {n['title']}")
        print(f"   æ¥æº: {n['source']} | åˆ†ç±»: {n['category']}")
        if n['summary']:
            print(f"   æ‘˜è¦: {n['summary'][:80]}...")
        if n['key_points']:
            for p in n['key_points']:
                print(f"   â€¢ {p}")
        print()
