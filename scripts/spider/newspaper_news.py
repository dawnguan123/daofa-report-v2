#!/usr/bin/env python3
"""
æ—¶æ”¿æ–°é—»è·å–å™¨ - Newspaper3k ç‰ˆ
ä½¿ç”¨ newspaper3k æ™ºèƒ½æå–æ–°é—»æ­£æ–‡
"""
import requests
import re
import json
import time
import sys
from datetime import datetime
from bs4 import BeautifulSoup
from newspaper import Article

# å°è¯•ä¸‹è½½ nltk æ•°æ®ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰
try:
    import nltk
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)
except:
    pass


class NewspaperNewsFetcher:
    """åŸºäº newspaper3k çš„æ–°é—»è·å–å™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def fetch_list(self, url, selector='.content_list li'):
        """è·å–æ–°é—»åˆ—è¡¨"""
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    çŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select(selector)
            
            news_list = []
            seen_urls = set()
            
            for item in items[:15]:
                link = item.find('a')
                if not link:
                    continue
                
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                if not href or not title or len(title) < 8:
                    continue
                
                # æ¸…ç†URL
                if href.startswith('//'):
                    full_url = 'https:' + href
                elif href.startswith('/'):
                    full_url = 'https://www.chinanews.com.cn' + href
                else:
                    full_url = href
                
                if full_url in seen_urls:
                    continue
                seen_urls.add(full_url)
                
                # æå–æ—¶é—´
                time_elem = item.find('span', class_='dd')
                time_str = time_elem.get_text(strip=True) if time_elem else datetime.now().strftime('%Y-%m-%d')
                
                news_list.append({
                    'title': title,
                    'url': full_url,
                    'time': time_str,
                    'category': self._guess_category(title),
                })
            
            return news_list
            
        except Exception as e:
            print(f"    è·å–åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def fetch_detail(self, news):
        """ä½¿ç”¨ newspaper3k æå–æ–°é—»è¯¦æƒ…"""
        try:
            article = Article(news['url'], language='zh')
            article.download()
            article.parse()
            
            # è·å–æ ‡é¢˜
            if article.title:
                news['title'] = article.title
            
            # è·å–æ­£æ–‡ï¼ˆå·²è‡ªåŠ¨å»é™¤å¹¿å‘Šå’Œå¯¼èˆªæ ï¼‰
            news['content'] = article.text
            
            # è·å–å‘å¸ƒæ—¶é—´
            if article.publish_date:
                news['time'] = article.publish_date.strftime('%Y-%m-%d %H:%M')
            
            # ç”Ÿæˆæ‘˜è¦
            news['summary'] = self._generate_summary(article.text)
            
            # æå–å…³é”®è¦ç‚¹
            news['key_points'] = self._extract_key_points(article.text, news['title'])
            
            # æ¥æº
            news['source'] = self._guess_source(news['url'])
            
            return news
            
        except Exception as e:
            print(f"    è¯¦æƒ…è·å–å¤±è´¥: {e}")
            # å¤‡ç”¨ï¼šä½¿ç”¨ç®€åŒ–çš„ requests è·å–
            return self._fetch_detail_backup(news)
    
    def _fetch_detail_backup(self, news):
        """å¤‡ç”¨è¯¦æƒ…è·å–æ–¹æ³•"""
        try:
            response = self.session.get(news['url'], timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                return news
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ­£æ–‡
            content_div = soup.find('div', class_='content')
            if content_div:
                paragraphs = content_div.find_all('p')
                texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30]
                news['content'] = ' '.join(texts)
            else:
                all_p = soup.find_all('p')
                texts = [p.get_text(strip=True) for p in all_p if len(p.get_text(strip=True)) > 30]
                news['content'] = ' '.join(texts[:10])
            
            news['summary'] = self._generate_summary(news.get('content', ''))
            news['key_points'] = self._extract_key_points(news.get('content', ''), news['title'])
            news['source'] = self._guess_source(news['url'])
            
            return news
            
        except Exception as e:
            print(f"    å¤‡ç”¨è·å–ä¹Ÿå¤±è´¥: {e}")
            return news
    
    def _generate_summary(self, content):
        """ç”Ÿæˆæ‘˜è¦"""
        if not content or len(content) < 50:
            return ""
        
        # å–å‰300å­—
        summary = content[:300]
        if len(content) > 300:
            summary += "..."
        
        return summary
    
    def _extract_key_points(self, content, title):
        """æå–å…³é”®è¦ç‚¹"""
        key_points = []
        
        if not content:
            return []
        
        # æå–äººå
        names = re.findall(r'ä¹ è¿‘å¹³|æå¼º|ä¸è–›ç¥¥|æå¸Œ|ç‹æ¯…|èµµä¹é™…', content)
        if names:
            key_points.append(f"æ¶‰åŠäººç‰©ï¼š{', '.join(set(names[:2]))}")
        
        # æå–æœºæ„
        orgs = re.findall(r'ä¸­å…±ä¸­å¤®|å›½åŠ¡é™¢|ä¸­å¤®å†›å§”|å›½å°åŠ|å·¥ä¿¡éƒ¨|ç§‘æŠ€éƒ¨|æ•™è‚²éƒ¨|å¸‚åœºç›‘ç®¡æ€»å±€', content)
        if orgs:
            key_points.append(f"æ¶‰åŠæœºæ„ï¼š{', '.join(set(orgs[:2]))}")
        
        # æå–å…³é”®äº‹ä»¶
        events = re.findall(r'(æœˆçƒæ¢æµ‹|è½½äººèˆªå¤©|åè…è´¥|ä¸¤å²¸|å»ºå†›|æ”¹é©|å‘å±•|ç§‘æŠ€è‡ªç«‹è‡ªå¼º)', content)
        if events:
            key_points.append(f"å…³é”®äº‹ä»¶ï¼š{', '.join(set(events[:2]))}")
        
        # æå–æ—¶é—´
        dates = re.findall(r'(\d+å¹´\d+æœˆ\d+æ—¥|\d+æœˆ\d+æ—¥)', content)
        if dates:
            key_points.append(f"æ—¶é—´ï¼š{dates[0]}")
        
        return key_points
    
    def _guess_category(self, title):
        """çŒœæµ‹åˆ†ç±»"""
        categories = {
            'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•'],
            'æ³•å¾‹': ['æ³•å¾‹', 'æ³•é™¢', 'æ£€å¯Ÿ', 'å¸æ³•', 'çŠ¯ç½ª', 'æœªæˆå¹´'],
            'ç»æµ': ['ç»æµ', 'GDP', 'CPI', 'ç»Ÿè®¡å±€', 'å¸‚åœº', 'æ¶ˆè´¹', 'äº§ä¸š'],
            'æ”¿æ²»': ['ä¹ è¿‘å¹³', 'æå¼º', 'ä¼šè®®', 'è®²è¯', 'å…šå»º', 'å†›é˜Ÿ', 'å»ºå†›', 'ä¸­å…±ä¸­å¤®'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'åŒ»ç–—', 'ç¤¾ä¿', 'å°±ä¸š', 'ä½æˆ¿'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'èˆªå¤©', 'æœˆçƒ', 'AI', 'åˆ›æ–°', 'è‡ªç«‹è‡ªå¼º'],
            'ä¸¤å²¸': ['å°æ¹¾', 'ä¸¤å²¸', 'å›½å°åŠ', 'å°æµ·', 'ç»Ÿä¸€'],
            'å¤–äº¤': ['å¤–äº¤', 'å¤–é•¿', 'è”åˆå›½', 'å›½é™…', 'å³°ä¼š']
        }
        
        for cat, keywords in categories.items():
            for kw in keywords:
                if kw in title:
                    return cat
        
        return 'æ—¶æ”¿'
    
    def _guess_source(self, url):
        """çŒœæµ‹æ¥æº"""
        if 'sina' in url:
            return 'æ–°æµªæ–°é—»'
        elif 'qq' in url:
            return 'è…¾è®¯æ–°é—»'
        elif 'ifeng' in url:
            return 'å‡¤å‡°æ–°é—»'
        elif 'people' in url:
            return 'äººæ°‘ç½‘'
        elif 'xinhuanet' in url:
            return 'æ–°åç½‘'
        elif 'chinanews' in url:
            return 'ä¸­å›½æ–°é—»ç½‘'
        elif 'inewsweek' in url:
            return 'ä¸­å›½æ–°é—»å‘¨åˆŠ'
        elif 'moe' in url:
            return 'æ•™è‚²éƒ¨'
        else:
            return 'å…¶ä»–åª’ä½“'
    
    def get_chinanews(self, max_news=5):
        """è·å–ä¸­å›½æ–°é—»ç½‘æ–°é—»"""
        print("  ğŸŒ æ­£åœ¨ä»ä¸­å›½æ–°é—»ç½‘è·å–...")
        
        # è·å–åˆ—è¡¨
        news_list = self.fetch_list(
            url='https://www.chinanews.com.cn/china/',
            selector='.content_list li'
        )
        
        if not news_list:
            # å°è¯•å¤‡ç”¨é€‰æ‹©å™¨
            news_list = self.fetch_list(
                url='https://www.chinanews.com.cn/gn/',
                selector='.dd_box li'
            )
        
        print(f"    è·å– {len(news_list)} æ¡æ ‡é¢˜")
        
        if not news_list:
            return []
        
        # ä¸°å¯Œå†…å®¹
        enriched = []
        for i, news in enumerate(news_list[:max_news], 1):
            print(f"    [{i}] æå–: {news['title'][:25]}...")
            enriched_news = self.fetch_detail(news)
            enriched.append(enriched_news)
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        return enriched


def fetch_news():
    """ä¾¿æ·å‡½æ•°"""
    fetcher = NewspaperNewsFetcher()
    return fetcher.get_chinanews(max_news=5)


if __name__ == "__main__":
    print("=" * 60)
    print(" Newspaper3k æ–°é—»è·å–å™¨")
    print("=" * 60)
    
    fetcher = NewspaperNewsFetcher()
    news_list = fetcher.get_chinanews(max_news=5)
    
    print(f"\n{'=' * 60}")
    print(f"è·å–åˆ° {len(news_list)} æ¡æ–°é—»:")
    print('=' * 60)
    
    for i, n in enumerate(news_list, 1):
        print(f"\n{i}. {n.get('title', 'æ— æ ‡é¢˜')[:50]}")
        print(f"   æ¥æº: {n.get('source', 'æœªçŸ¥')} | åˆ†ç±»: {n.get('category', 'æœªçŸ¥')}")
        if n.get('summary'):
            print(f"   æ‘˜è¦: {n['summary'][:80]}...")
        if n.get('key_points'):
            for p in n['key_points']:
                print(f"   â€¢ {p}")
    
    # ä¿å­˜ä¸º JSON
    output = {
        'date': datetime.now().strftime('%Y-%m-%d'),
        'count': len(news_list),
        'news': news_list
    }
    
    with open('output/chinanews_latest.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å·²ä¿å­˜åˆ° output/chinanews_latest.json")
