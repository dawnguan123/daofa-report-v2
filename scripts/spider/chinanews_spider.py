#!/usr/bin/env python3
"""
ä¸­å›½æ–°é—»ç½‘æ–°é—»çˆ¬è™« - æœ€ç»ˆç‰ˆ
"""
import requests
import re
import json
from datetime import datetime
from bs4 import BeautifulSoup
import time

class ChinaNewsSpider:
    def __init__(self):
        self.base_url = "https://www.chinanews.com.cn/"
        self.china_url = "https://www.chinanews.com.cn/china/"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        }
    
    def fix_url(self, href):
        """ä¿®å¤URLæ ¼å¼"""
        if not href:
            return ""
        if href.startswith('http'):
            return href
        if href.startswith('//'):
            return 'https:' + href
        if href.startswith('/'):
            return 'https://www.chinanews.com.cn' + href
        return 'https://www.chinanews.com.cn/' + href
    
    def fetch_title_list(self):
        """è·å–æ–°é—»æ ‡é¢˜åˆ—è¡¨"""
        try:
            print("  ğŸŒ æ­£åœ¨è·å–æ–°é—»åˆ—è¡¨...")
            response = requests.get(self.china_url, headers=self.headers, timeout=20)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_list = []
            seen = set()
            
            links = soup.select('a[href*="2026/"]')
            for link in links[:30]:
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                if not href or not title or len(title) < 10:
                    continue
                
                # å¤„ç† // å‰ç¼€
                if href.startswith('//'):
                    full_url = 'https:' + href
                elif href.startswith('/'):
                    full_url = 'https://www.chinanews.com.cn' + href
                else:
                    full_url = href
                
                if full_url in seen:
                    continue
                seen.add(full_url)
                
                news_list.append({
                    'title': title,
                    'url': full_url,
                    'source': 'ä¸­å›½æ–°é—»ç½‘',
                    'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                    'category': 'æ—¶æ”¿',
                })
            
            print(f"    è·å– {len(news_list)} æ¡æ ‡é¢˜")
            return news_list[:10]
            
        except Exception as e:
            print(f"    é”™è¯¯: {e}")
            return []
    
    def fetch_detail(self, url):
        """è·å–è¯¦æƒ…é¡µå†…å®¹"""
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'  # å¼ºåˆ¶utf-8ç¼–ç 
            if response.status_code != 200:
                return "", []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            all_p = soup.find_all('p')
            
            if not all_p:
                return "", []
            
            texts = [p.get_text(strip=True) for p in all_p]
            texts = [t for t in texts if len(t) > 15]
            content = ' '.join(texts)
            
            # ç”Ÿæˆæ‘˜è¦å’Œè¦ç‚¹
            summary = content[:300] + ("..." if len(content) > 300 else "")
            
            key_points = []
            
            # æå–è¦ç´ 
            names = re.findall(r'ä¹ è¿‘å¹³|æå¼º|ä¸è–›ç¥¥|æå¸Œ|æœ±å‡¤è²|é©¬å¸…è', content)
            if names:
                key_points.append(f"äººç‰©ï¼š{', '.join(set(names))}")
            
            orgs = re.findall(r'ä¸­å…±ä¸­å¤®|å›½åŠ¡é™¢|ä¸­å¤®å†›å§”|å›½å°åŠ|å·¥ä¿¡éƒ¨|ç§‘æŠ€éƒ¨|ä½å»ºéƒ¨|å¸‚åœºç›‘ç®¡æ€»å±€|çŸ¥è¯†äº§æƒå±€|è½½äººèˆªå¤©å·¥ç¨‹åŠå…¬å®¤', content)
            if orgs:
                key_points.append(f"æœºæ„ï¼š{', '.join(set(orgs[:3]))}")
            
            events = re.findall(r'æœˆçƒæ¢æµ‹|è½½äººèˆªå¤©|åè…è´¥|ä¸¤å²¸|ç§‘æŠ€æœåŠ¡|æ ‡å‡†ä½“ç³»|å»ºå†›|å»ºå†›', content)
            if events:
                key_points.append(f"äº‹ä»¶ï¼š{', '.join(set(events))}")
            
            dates = re.findall(r'(\d+å¹´\d+æœˆ\d+æ—¥|\d+æœˆ\d+æ—¥|\d+æ—¥)', content)
            if dates:
                key_points.append(f"æ—¶é—´ï¼š{dates[0]}")
            
            return summary, key_points[:3]
            
        except Exception as e:
            return "", []
    
    def get_news(self, max_news=5):
        """è·å–æ–°é—»"""
        news_list = self.fetch_title_list()
        if not news_list:
            return []
        
        print(f"\n  ğŸ“° ä¸‹é’»åˆ°è¯¦æƒ…é¡µ...")
        for i, news in enumerate(news_list[:max_news], 1):
            print(f"    [{i}] {news['title'][:25]}...")
            summary, key_points = self.fetch_detail(news['url'])
            news['summary'] = summary if summary else news['title']
            news['key_points'] = key_points
            time.sleep(0.3)
        
        return news_list[:max_news]

if __name__ == "__main__":
    spider = ChinaNewsSpider()
    news = spider.get_news(max_news=3)
    
    print(f"\n{'='*60}")
    for i, n in enumerate(news, 1):
        print(f"\n{i}. {n['title']}")
        print(f"   æ‘˜è¦: {n['summary'][:100]}...")
        if n['key_points']:
            for p in n['key_points']:
                print(f"   â€¢ {p}")
