#!/usr/bin/env python3
"""
æ—¶æ”¿æ–°é—»è·å–å™¨ - æ··åˆæ–¹æ¡ˆ
ä¸»æ–¹æ¡ˆ: Tavily APIï¼ˆå·²éªŒè¯å¯ç”¨ï¼‰
å¤‡é€‰æ–¹æ¡ˆ: newspaper3kï¼ˆé€‚ç”¨äºæ— åçˆ¬ç½‘ç«™ï¼‰
"""
import requests
import re
import json
import time
from datetime import datetime
from bs4 import BeautifulSoup
from tavily import TavilyClient

# é…ç½®
TAVILY_API_KEY = "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"


class HybridNewsFetcher:
    """æ··åˆæ–°é—»è·å–å™¨"""
    
    def __init__(self, api_key=None):
        self.api_key = api_key or TAVILY_API_KEY
        self.tavily_client = TavilyClient(api_key=self.api_key)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
    
    def get_news_via_tavily(self, max_news=5):
        """æ–¹æ¡ˆ1: Tavily API æœç´¢"""
        print("  ğŸ” ä½¿ç”¨ Tavily API æœç´¢...")
        
        queries = [
            "2026å¹´2æœˆ ä¸­å›½æ—¶æ”¿æ–°é—» ä¹ è¿‘å¹³",
            "2026å¹´2æœˆ ä¸­å›½æ”¿åºœ å›½åŠ¡é™¢ æ”¿ç­–",
        ]
        
        all_results = []
        
        for query in queries[:2]:
            print(f"    æœç´¢: {query[:30]}...")
            try:
                response = self.tavily_client.search(
                    query=query,
                    max_results=5,
                    include_answer=True
                )
                results = response.get('results', [])
                all_results.extend(results)
                print(f"      è·å– {len(results)} æ¡")
            except Exception as e:
                print(f"      æœç´¢å¤±è´¥: {e}")
        
        # å»é‡
        seen_urls = set()
        unique_results = []
        for r in all_results:
            url = r.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(r)
        
        print(f"  ğŸ“Š å»é‡å: {len(unique_results)} æ¡")
        
        # ä¸°å¯Œå†…å®¹
        news_list = []
        for i, r in enumerate(unique_results[:max_news], 1):
            print(f"  [{i}] å¤„ç†: {r.get('title', '')[:30]}...")
            
            news = {
                'title': r.get('title', ''),
                'url': r.get('url', ''),
                'source': self._guess_source(r.get('url', '')),
                'time': datetime.now().strftime('%Y-%m-%d %H:%M'),
                'category': self._guess_category(r.get('title', '')),
                'content': r.get('content', '') or r.get('answer', ''),
                'summary': self._generate_summary(r.get('answer', '')),
                'key_points': self._extract_key_points(
                    r.get('content', '') or r.get('answer', ''),
                    r.get('title', '')
                )
            }
            news_list.append(news)
        
        return news_list
    
    def get_news_via_requests(self, url, selector, max_news=5):
        """æ–¹æ¡ˆ2: ç›´æ¥ requestsï¼ˆé€‚ç”¨äºæ— åçˆ¬ç½‘ç«™ï¼‰"""
        print(f"  ğŸŒ ç›´æ¥è®¿é—®: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"    çŠ¶æ€ç : {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select(selector)
            
            news_list = []
            seen_urls = set()
            
            for item in items[:max_news * 2]:
                link = item.find('a')
                if not link:
                    continue
                
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                if not href or not title or len(title) < 10:
                    continue
                
                if href.startswith('//'):
                    full_url = 'https:' + href
                elif href.startswith('/'):
                    full_url = 'https://www.chinanews.com.cn' + href
                else:
                    full_url = href
                
                if full_url in seen_urls:
                    continue
                seen_urls.add(full_url)
                
                # è·å–è¯¦æƒ…
                content = self._fetch_content(full_url)
                
                news = {
                    'title': title,
                    'url': full_url,
                    'source': self._guess_source(full_url),
                    'time': datetime.now().strftime('%Y-%m-%d'),
                    'category': self._guess_category(title),
                    'content': content,
                    'summary': self._generate_summary(content),
                    'key_points': self._extract_key_points(content, title)
                }
                news_list.append(news)
                print(f"    [{len(news_list)}] {title[:25]}...")
                time.sleep(0.5)
                
                if len(news_list) >= max_news:
                    break
            
            return news_list
            
        except Exception as e:
            print(f"    é”™è¯¯: {e}")
            return []
    
    def _fetch_content(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                return ""
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•æ‰¾åˆ°æ­£æ–‡
            content_div = soup.find('div', class_='content') or \
                         soup.find('div', class_='article') or \
                         soup.find('article')
            
            if content_div:
                paragraphs = content_div.find_all('p')
                texts = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30]
                return ' '.join(texts[:15])
            
            # å¤‡ç”¨ï¼šæ‰€æœ‰ p æ ‡ç­¾
            all_p = soup.find_all('p')
            texts = [p.get_text(strip=True) for p in all_p if len(p.get_text(strip=True)) > 30]
            return ' '.join(texts[:10])
            
        except:
            return ""
    
    def _generate_summary(self, content):
        """ç”Ÿæˆæ‘˜è¦"""
        if not content or len(content) < 50:
            return ""
        return content[:300] + ("..." if len(content) > 300 else "")
    
    def _extract_key_points(self, content, title):
        """æå–å…³é”®è¦ç‚¹"""
        key_points = []
        
        if not content:
            return key_points
        
        names = re.findall(r'ä¹ è¿‘å¹³|æå¼º|ä¸è–›ç¥¥|æå¸Œ|ç‹æ¯…|èµµä¹é™…', content)
        if names:
            key_points.append(f"æ¶‰åŠäººç‰©ï¼š{', '.join(set(names[:2]))}")
        
        orgs = re.findall(r'ä¸­å…±ä¸­å¤®|å›½åŠ¡é™¢|ä¸­å¤®å†›å§”|å›½å°åŠ|å·¥ä¿¡éƒ¨|ç§‘æŠ€éƒ¨|æ•™è‚²éƒ¨', content)
        if orgs:
            key_points.append(f"æ¶‰åŠæœºæ„ï¼š{', '.join(set(orgs[:2]))}")
        
        events = re.findall(r'(æœˆçƒæ¢æµ‹|è½½äººèˆªå¤©|åè…è´¥|ä¸¤å²¸|å»ºå†›|æ”¹é©|å‘å±•)', content)
        if events:
            key_points.append(f"å…³é”®äº‹ä»¶ï¼š{', '.join(set(events[:2]))}")
        
        return key_points
    
    def _guess_category(self, title):
        """çŒœæµ‹åˆ†ç±»"""
        categories = {
            'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å­¦ç”Ÿ', 'æ•™å¸ˆ', 'è€ƒè¯•'],
            'æ³•å¾‹': ['æ³•å¾‹', 'æ³•é™¢', 'æ£€å¯Ÿ', 'å¸æ³•', 'çŠ¯ç½ª'],
            'ç»æµ': ['ç»æµ', 'GDP', 'CPI', 'ç»Ÿè®¡å±€', 'å¸‚åœº', 'æ¶ˆè´¹'],
            'æ”¿æ²»': ['ä¹ è¿‘å¹³', 'æå¼º', 'ä¼šè®®', 'è®²è¯', 'å…šå»º', 'å†›é˜Ÿ', 'ä¸­å…±ä¸­å¤®'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'åŒ»ç–—', 'ç¤¾ä¿', 'å°±ä¸š'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'èˆªå¤©', 'æœˆçƒ', 'AI', 'åˆ›æ–°'],
            'ä¸¤å²¸': ['å°æ¹¾', 'ä¸¤å²¸', 'å›½å°åŠ', 'å°æµ·'],
            'å¤–äº¤': ['å¤–äº¤', 'å¤–é•¿', 'è”åˆå›½', 'å›½é™…']
        }
        
        for cat, keywords in categories.items():
            for kw in keywords:
                if kw in title:
                    return cat
        return 'æ—¶æ”¿'
    
    def _guess_source(self, url):
        """çŒœæµ‹æ¥æº"""
        if 'sina' in url:
            return 'æ–°æµªæ–°é—»'
        elif 'qq' in url:
            return 'è…¾è®¯æ–°é—»'
        elif 'people' in url:
            return 'äººæ°‘ç½‘'
        elif 'xinhuanet' in url:
            return 'æ–°åç½‘'
        elif 'chinanews' in url:
            return 'ä¸­å›½æ–°é—»ç½‘'
        elif 'moe' in url:
            return 'æ•™è‚²éƒ¨'
        else:
            return 'å…¶ä»–åª’ä½“'
    
    def get_political_news(self, max_news=5, method='tavily'):
        """è·å–æ—¶æ”¿æ–°é—»ä¸»å…¥å£"""
        print(f"\nğŸ“° å¼€å§‹è·å–æ—¶æ”¿æ–°é—» (æ–¹æ¡ˆ: {method})...")
        
        if method == 'tavily':
            return self.get_news_via_tavily(max_news)
        else:
            return self.get_news_via_requests(
                url='https://www.chinanews.com.cn/china/',
                selector='.content_list li',
                max_news=max_news
            )


def fetch_news():
    """ä¾¿æ·å‡½æ•° - ä½¿ç”¨ Tavily"""
    fetcher = HybridNewsFetcher()
    return fetcher.get_political_news(max_news=5, method='tavily')


if __name__ == "__main__":
    fetcher = HybridNewsFetcher()
    news_list = fetcher.get_political_news(max_news=5, method='tavily')
    
    print(f"\nâœ… è·å–åˆ° {len(news_list)} æ¡æ–°é—»")
    
    for i, n in enumerate(news_list, 1):
        print(f"\n{i}. {n['title'][:50]}")
        print(f"   æ¥æº: {n['source']} | åˆ†ç±»: {n['category']}")
        if n['summary']:
            print(f"   æ‘˜è¦: {n['summary'][:60]}...")
